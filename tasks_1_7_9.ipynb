{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aD8TOhSq-J9k"
      },
      "outputs": [],
      "source": [
        "# ! pip install python-doctr mplcursors matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_N07RCEFnuJg",
        "outputId": "86e74b8b-e294-422d-febc-d35da446024e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 7: Determine the length of the agreement"
      ],
      "metadata": {
        "id": "8f7xNHuCoLXi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwYB9JhBBgTU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "root_dir = 'drive/MyDrive/NLP/OCR_output'\n",
        "\n",
        "data_entries = []\n",
        "\n",
        "for state_folder in os.listdir(root_dir):\n",
        "  state_path = os.path.join(root_dir, state_folder)\n",
        "  for filename in os.listdir(state_path):\n",
        "    if filename.endswith('.json'):\n",
        "        file_path = os.path.join(state_path, filename)\n",
        "\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            json_output = json.load(f)\n",
        "\n",
        "        num_pages = len(json_output['pages'])\n",
        "        total_words = 0\n",
        "\n",
        "        for page in json_output['pages']:\n",
        "            for block in page['blocks']:\n",
        "                for line in block['lines']:\n",
        "                    total_words += len(line['words'])\n",
        "\n",
        "        data_entries.append({\n",
        "            'State': state_folder,\n",
        "            'Filename': filename,\n",
        "            'Num_Pages': num_pages,\n",
        "            'Total_Words': total_words\n",
        "        })\n",
        "\n",
        "df = pd.DataFrame(data_entries)\n",
        "\n",
        "df.to_csv('drive/MyDrive/NLP/Agreements_metadata.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 9: Analysis of Frequency of Recurring Clauses"
      ],
      "metadata": {
        "id": "m9VKu0TprQmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install sentence-transformers hdbscan"
      ],
      "metadata": {
        "id": "FsQmkn_zraI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import hdbscan\n",
        "\n",
        "ROOT_DIR = 'drive/MyDrive/NLP/OCR_output'\n",
        "MIN_WORDS_IN_CLAUSE = 10  # ignoring short lines (headers, page numbers)\n",
        "MIN_CLUSTER_SIZE = 2      # a clause must appear in at least 2 docs to be a \"cluster\"\n",
        "\n",
        "documents_data = [] # stores metadata for every clause found\n",
        "corpus_sentences = [] # stores the actual text for embedding\n",
        "\n",
        "for state_folder in os.listdir(ROOT_DIR):\n",
        "  state_path = os.path.join(ROOT_DIR, state_folder)\n",
        "\n",
        "  for filename in os.listdir(state_path):\n",
        "    if not filename.endswith('.json'): continue\n",
        "\n",
        "    file_path = os.path.join(state_path, filename)\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # iterate through blocks (paragraphs)\n",
        "    for page_num, page in enumerate(data['pages']):\n",
        "        for block in page['blocks']:\n",
        "\n",
        "            # reconstruct text from the block\n",
        "            lines_text = []\n",
        "            for line in block['lines']:\n",
        "                words = [w['value'] for w in line['words']]\n",
        "                lines_text.append(\" \".join(words))\n",
        "\n",
        "            block_text = \" \".join(lines_text).strip()\n",
        "\n",
        "            if len(block_text.split()) >= MIN_WORDS_IN_CLAUSE:\n",
        "                corpus_sentences.append(block_text)\n",
        "                documents_data.append({\n",
        "                    'State': state_folder,\n",
        "                    'Filename': filename,\n",
        "                    'Page': page_num + 1,\n",
        "                    'Text': block_text\n",
        "                })\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2') # fast, lightweight model\n",
        "embeddings = model.encode(corpus_sentences, show_progress_bar=True)\n",
        "\n",
        "clusterer = hdbscan.HDBSCAN(\n",
        "    min_cluster_size=MIN_CLUSTER_SIZE,\n",
        "    min_samples=1,\n",
        "    metric='euclidean'\n",
        ")\n",
        "cluster_labels = clusterer.fit_predict(embeddings)\n",
        "\n",
        "df = pd.DataFrame(documents_data)\n",
        "df['Cluster_ID'] = cluster_labels\n",
        "\n",
        "cluster_counts = df['Cluster_ID'].value_counts()\n",
        "total_docs = df['Filename'].nunique()\n",
        "\n",
        "def classify_frequency(cluster_id, count):\n",
        "    if cluster_id == -1:\n",
        "        return \"Rarely (Unique/Custom)\"\n",
        "\n",
        "    # If a clause appears in > 80% of documents, it's boilerplate\n",
        "    elif count >= (total_docs * 0.8):\n",
        "        return \"Always (Boilerplate)\"\n",
        "    else:\n",
        "        return \"Often (Standard Clause)\"\n",
        "\n",
        "df['Frequency_Label'] = df.apply(\n",
        "    lambda x: classify_frequency(x['Cluster_ID'], cluster_counts[x['Cluster_ID']]),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "df.sort_values(by='Cluster_ID', inplace=True)\n",
        "\n",
        "df.to_csv('drive/MyDrive/NLP/clause_analysis.csv', index=False)\n"
      ],
      "metadata": {
        "id": "f6L1zguLresy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1: Identification of Areas of Cooperation"
      ],
      "metadata": {
        "id": "n6CYMFn8u3rC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers torch"
      ],
      "metadata": {
        "id": "TdEy_OE_u-d9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "\n",
        "ROOT_DIR = 'drive/MyDrive/NLP/OCR_output'\n",
        "\n",
        "CANDIDATE_LABELS = [\n",
        "    \"Culture & Arts\",\n",
        "    \"Education & Students\",\n",
        "    \"Trade & Economic Development\",\n",
        "    \"Environment & Green Energy\",\n",
        "    \"Tourism\",\n",
        "    \"Infrastructure & Transport\",\n",
        "    \"Technology & Innovation\",\n",
        "    \"Public Health\",\n",
        "    \"Friendship & Goodwill\"\n",
        "]\n",
        "\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "results = []\n",
        "\n",
        "for state_folder in os.listdir(ROOT_DIR):\n",
        "    state_path = os.path.join(ROOT_DIR, state_folder)\n",
        "    if not os.path.isdir(state_path): continue\n",
        "\n",
        "    for filename in os.listdir(state_path):\n",
        "        if not filename.endswith('.json'): continue\n",
        "\n",
        "        file_path = os.path.join(state_path, filename)\n",
        "\n",
        "        try:\n",
        "            # Load OCR Data\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            full_text_list = []\n",
        "            for page in data['pages']:\n",
        "                for block in page['blocks']:\n",
        "                    for line in block['lines']:\n",
        "                        words = [w['value'] for w in line['words']]\n",
        "                        full_text_list.append(\" \".join(words))\n",
        "\n",
        "            full_text = \" \".join(full_text_list)\n",
        "\n",
        "            truncated_text = full_text[:3000]\n",
        "\n",
        "            prediction = classifier(truncated_text, CANDIDATE_LABELS, multi_label=True)\n",
        "\n",
        "            scores = dict(zip(prediction['labels'], prediction['scores']))\n",
        "\n",
        "            # Only keep topics with > 50% confidence\n",
        "            active_topics = [label for label, score in scores.items() if score > 0.5]\n",
        "\n",
        "            # If nothing scored high, grab the top 1\n",
        "            if not active_topics:\n",
        "                active_topics = [prediction['labels'][0]]\n",
        "\n",
        "            results.append({\n",
        "                'State': state_folder,\n",
        "                'Filename': filename,\n",
        "                'Detected_Topics': \", \".join(active_topics),\n",
        "                'Top_Topic': prediction['labels'][0],\n",
        "                'Top_Score': round(prediction['scores'][0], 3)\n",
        "            })\n",
        "\n",
        "            print(f\"Processed {filename}: {active_topics}\")\n",
        "\n",
        "            df = pd.DataFrame(results)\n",
        "\n",
        "            output_file = 'drive/MyDrive/NLP/cooperation_areas.csv'\n",
        "            df.to_csv(output_file, index=False)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {filename}: {e}\")\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "output_file = 'drive/MyDrive/NLP/cooperation_areas.csv'\n",
        "df.to_csv(output_file, index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "g2_Hk3Ayu-gR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}