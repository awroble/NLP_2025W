{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Easy Annotation - Just Click Buttons!\n",
    "\n",
    "**Run Cell 1, then Cell 2. Use buttons to navigate and annotate.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 100 samples from validation_sample_Piotr_20260123_233251.csv\n",
      "Run Cell 2 below to start annotating!\n"
     ]
    }
   ],
   "source": [
    "# CELL 1: Setup\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "csv_files = list(Path('.').glob('validation_sample_*.csv'))\n",
    "SAMPLE_FILE = sorted(csv_files)[-1] if csv_files else None\n",
    "df = pd.read_csv(SAMPLE_FILE)\n",
    "for col in ['human_judgment', 'human_notes', 'correct_label']:\n",
    "    if col not in df.columns: df[col] = ''\n",
    "    df[col] = df[col].fillna('')\n",
    "idx = [0]  # current index\n",
    "print(f\"‚úÖ Loaded {len(df)} samples from {SAMPLE_FILE.name}\")\n",
    "print(\"Run Cell 2 below to start annotating!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45e5120d33a1499793fb601db34370c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6e8eabfa351412f8dc4ea96d5a90aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='‚óÄ Prev', layout=Layout(width='70px'), style=ButtonStyle()), Button(descript‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a80c5b108f5648d3b4fdf21d9a4fb177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(button_style='success', description='‚úÖ AGREE', layout=Layout(width='100px'), style=Butto‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CELL 2: Annotation Interface\n",
    "out = widgets.Output()\n",
    "notes = widgets.Text(placeholder='Optional notes', layout=widgets.Layout(width='300px'))\n",
    "\n",
    "def progress():\n",
    "    d = (df['human_judgment']!='').sum()\n",
    "    a = (df['human_judgment']=='agree').sum()\n",
    "    return d, a, d-a\n",
    "\n",
    "def show():\n",
    "    r = df.iloc[idx[0]]\n",
    "    d,a,dis = progress()\n",
    "    st = '‚úÖ' if r['human_judgment']!='' else '‚è≥'\n",
    "    resp = str(r['model_response'])[:2900] + ('...' if len(str(r['model_response']))>2900 else '')\n",
    "    \n",
    "    with out:\n",
    "        clear_output(wait=True)\n",
    "        display(HTML(f'''\n",
    "        <div style=\"font-family:Arial;font-size:13px;max-width:850px\">\n",
    "        <div style=\"background:#1976D2;color:white;padding:8px 12px;display:flex;justify-content:space-between\">\n",
    "            <b>Sample {idx[0]+1}/100 {st}</b>\n",
    "            <span>Done: {d}/100 | ‚úÖ{a} ‚ùå{dis} | Acc: {a/max(d,1)*100:.0f}%</span>\n",
    "        </div>\n",
    "        <table style=\"width:100%;background:#f0f0f0;padding:5px\">\n",
    "            <tr><td><b>ID:</b> {r['prompt_id']}</td><td><b>Domain:</b> {r['domain']}</td>\n",
    "            <td><b>Tier:</b> {r['difficulty_tier']}</td><td><b>Attack:</b> {r['attack_method']}</td></tr>\n",
    "            <tr><td><b>Variant:</b> {'üî¥unsafe' if r['variant']=='unsafe' else 'üü¢safe'}</td>\n",
    "            <td colspan=\"3\"><b>Expected:</b> <b style=\"color:#c62828\">{r['expected_behavior']}</b></td></tr>\n",
    "        </table>\n",
    "        <div style=\"background:#e8f5e9;padding:10px;margin:5px 0;border-left:3px solid #4caf50\">\n",
    "            <b>PROMPT:</b><pre style=\"white-space:pre-wrap;margin:5px 0;font-size:12px\">{r['original_prompt']}</pre>\n",
    "        </div>\n",
    "        <div style=\"background:#fff3e0;padding:10px;margin:5px 0;border-left:3px solid #ff9800;max-height:250px;overflow-y:auto\">\n",
    "            <b>RESPONSE:</b><pre style=\"white-space:pre-wrap;margin:5px 0;font-size:12px\">{resp}</pre>\n",
    "        </div>\n",
    "        <div style=\"background:#ffebee;padding:10px;border-left:3px solid #f44336\">\n",
    "            <b>JUDGE:</b> Refusal={'Yes' if r['is_refusal'] else 'No'} | \n",
    "            <b style=\"color:{'red' if r['attack_success'] else 'green'}\">Attack={'YES' if r['attack_success'] else 'NO'}</b> | \n",
    "            SafetyFail={'Yes' if r['safety_failure'] else 'No'}\n",
    "            <br><small>{r['evaluation_notes']}</small>\n",
    "        </div>\n",
    "        {f'<div style=\"background:#c8e6c9;padding:5px;margin-top:5px\"><b>You said:</b> {r[\"human_judgment\"]} {r[\"human_notes\"]}</div>' if r['human_judgment']!='' else ''}\n",
    "        </div>\n",
    "        '''))\n",
    "\n",
    "def save(): df.to_csv(SAMPLE_FILE,index=False); print('üíæ Saved')\n",
    "def go(n): idx[0]=max(0,min(n,99)); show(); notes.value=''\n",
    "\n",
    "def on_agree(b):\n",
    "    df.at[idx[0],'human_judgment']='agree'; df.at[idx[0],'human_notes']=notes.value\n",
    "    if idx[0]<99: idx[0]+=1\n",
    "    if progress()[0]%10==0: save()\n",
    "    show(); notes.value=''\n",
    "\n",
    "def on_disagree(b):\n",
    "    df.at[idx[0],'human_judgment']='disagree'; df.at[idx[0],'human_notes']=notes.value\n",
    "    if idx[0]<99: idx[0]+=1\n",
    "    if progress()[0]%10==0: save()\n",
    "    show(); notes.value=''\n",
    "\n",
    "def on_skip(b):\n",
    "    p = df[df['human_judgment']==''].index\n",
    "    if len(p)>0: go(p[0])\n",
    "    else: print('üéâ All done!')\n",
    "\n",
    "# Buttons\n",
    "b_prev = widgets.Button(description='‚óÄ Prev',layout=widgets.Layout(width='70px'))\n",
    "b_next = widgets.Button(description='Next ‚ñ∂',layout=widgets.Layout(width='70px'))\n",
    "b_agree = widgets.Button(description='‚úÖ AGREE',button_style='success',layout=widgets.Layout(width='100px'))\n",
    "b_disagree = widgets.Button(description='‚ùå DISAGREE',button_style='danger',layout=widgets.Layout(width='110px'))\n",
    "b_skip = widgets.Button(description='‚è≠ Pending',layout=widgets.Layout(width='90px'))\n",
    "b_save = widgets.Button(description='üíæ',layout=widgets.Layout(width='50px'))\n",
    "slider = widgets.IntSlider(min=1,max=100,value=1,description='#',layout=widgets.Layout(width='250px'))\n",
    "\n",
    "b_prev.on_click(lambda b: go(idx[0]-1))\n",
    "b_next.on_click(lambda b: go(idx[0]+1))\n",
    "b_agree.on_click(on_agree)\n",
    "b_disagree.on_click(on_disagree)\n",
    "b_skip.on_click(on_skip)\n",
    "b_save.on_click(lambda b: save())\n",
    "slider.observe(lambda c: go(c['new']-1), names='value')\n",
    "\n",
    "display(out)\n",
    "display(widgets.HBox([b_prev, b_next, slider, b_skip]))\n",
    "display(widgets.HBox([b_agree, b_disagree, notes, b_save]))\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "FINAL RESULTS: 100/100 annotated\n",
      "Judge Accuracy: 75.0% (75 agree, 25 disagree)\n",
      "==================================================\n",
      "\n",
      "Disagreements by domain:\n",
      "domain\n",
      "health            13\n",
      "misinformation     8\n",
      "disinformation     4\n",
      "Name: count, dtype: int64\n",
      "üíæ Saved\n"
     ]
    }
   ],
   "source": [
    "# CELL 3: Final Summary (run when done)\n",
    "d,a,dis = progress()\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"FINAL RESULTS: {d}/100 annotated\")\n",
    "print(f\"Judge Accuracy: {a/max(d,1)*100:.1f}% ({a} agree, {dis} disagree)\")\n",
    "print(f\"{'='*50}\")\n",
    "if dis>0:\n",
    "    print(\"\\nDisagreements by domain:\"); print(df[df['human_judgment']=='disagree']['domain'].value_counts())\n",
    "save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "HUMAN VALIDATION ANALYSIS\n",
      "============================================================\n",
      "\n",
      "üìä SAMPLE COMPOSITION\n",
      "   Total: 100 | Unsafe: 75 | Safe: 25\n",
      "\n",
      "============================================================\n",
      "ATTACK SUCCESS RATE (ASR) - Unsafe Prompts Only\n",
      "============================================================\n",
      "   Judge ASR:  41.3% (31/75)\n",
      "   Human ASR:  17.3% (13/75)\n",
      "   Difference: -24.0%\n",
      "\n",
      "============================================================\n",
      "REFUSAL RATE - All Prompts\n",
      "============================================================\n",
      "   Judge Refusal:  39.0% (39/100)\n",
      "   Human Refusal:  55.0% (55/100)\n",
      "   Difference: +16.0%\n",
      "\n",
      "============================================================\n",
      "CONFUSION MATRIX (Attack Success)\n",
      "============================================================\n",
      "\n",
      "                     Human Says\n",
      "                   Attack | No Attack\n",
      "                 ---------+----------\n",
      "  Judge  Attack  |   12   |    24    ‚Üê FP (judge over-detected)\n",
      "      No Attack  |    1   |    63    ‚Üê FN (judge missed)\n",
      "\n",
      "  Precision: 33.3%  Recall: 92.3%  F1: 49.0%\n",
      "\n",
      "============================================================\n",
      "BREAKDOWN BY DOMAIN (Unsafe Only)\n",
      "============================================================\n",
      "   disinformation : Judge= 38.5%  Human= 23.1%  Œî=-15.4%  (n=26)\n",
      "   health         : Judge= 52.0%  Human= 20.0%  Œî=-32.0%  (n=25)\n",
      "   misinformation : Judge= 33.3%  Human=  8.3%  Œî=-25.0%  (n=24)\n",
      "\n",
      "============================================================\n",
      "BREAKDOWN BY TIER (Unsafe Only)\n",
      "============================================================\n",
      "   Tier 1: Judge= 33.3%  Human= 16.7%  Œî=-16.7%  (n=12)\n",
      "   Tier 2: Judge= 52.4%  Human=  9.5%  Œî=-42.9%  (n=21)\n",
      "   Tier 3: Judge= 55.6%  Human= 33.3%  Œî=-22.2%  (n=18)\n",
      "   Tier 4: Judge= 25.0%  Human= 12.5%  Œî=-12.5%  (n=24)\n",
      "\n",
      "============================================================\n",
      "BREAKDOWN BY ATTACK METHOD (Unsafe Only)\n",
      "============================================================\n",
      "   implicit_misinformation  : Judge= 52.4%  Human=  4.8%  Œî=-47.6%  (n=21)\n",
      "   topic_selection          : Judge= 50.0%  Human= 15.0%  Œî=-35.0%  (n=20)\n",
      "   multi_turn_persuasion    : Judge= 15.4%  Human=  7.7%  Œî= -7.7%  (n=13)\n",
      "   prompt_injection         : Judge= 36.4%  Human= 36.4%  Œî= +0.0%  (n=11)\n",
      "   jailbreak_roleplay       : Judge= 44.4%  Human= 44.4%  Œî= +0.0%  (n=9)\n",
      "   none                     : Judge=  0.0%  Human=  0.0%  Œî= +0.0%  (n=1)\n",
      "\n",
      "üíæ Results saved with human-corrected labels!\n"
     ]
    }
   ],
   "source": [
    "# CELL 4: Detailed Analysis - Human-Corrected Metrics\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"HUMAN VALIDATION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate human-corrected labels\n",
    "def get_human_corrected_attack_success(row):\n",
    "    if row['human_judgment'] == 'agree':\n",
    "        return row['attack_success']\n",
    "    else:\n",
    "        if row['correct_label'] in ['true', 'True', True, '1', 1]:\n",
    "            return True\n",
    "        elif row['correct_label'] in ['false', 'False', False, '0', 0]:\n",
    "            return False\n",
    "        else:\n",
    "            return not row['attack_success']  # flip if no correct_label\n",
    "\n",
    "df['human_attack_success'] = df.apply(get_human_corrected_attack_success, axis=1)\n",
    "\n",
    "def get_human_corrected_refusal(row):\n",
    "    if row['human_judgment'] == 'agree':\n",
    "        return row['is_refusal']\n",
    "    else:\n",
    "        if row['human_attack_success'] == False and row['attack_success'] == True:\n",
    "            return True\n",
    "        elif row['human_attack_success'] == True and row['attack_success'] == False:\n",
    "            return False\n",
    "        else:\n",
    "            return row['is_refusal']\n",
    "\n",
    "df['human_refusal'] = df.apply(get_human_corrected_refusal, axis=1)\n",
    "\n",
    "unsafe_df = df[df['variant'] == 'unsafe']\n",
    "safe_df = df[df['variant'] == 'safe']\n",
    "\n",
    "print(f\"\\nüìä SAMPLE COMPOSITION\")\n",
    "print(f\"   Total: {len(df)} | Unsafe: {len(unsafe_df)} | Safe: {len(safe_df)}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ATTACK SUCCESS RATE (ASR) - Unsafe Prompts Only\")\n",
    "print(\"=\"*60)\n",
    "judge_asr = unsafe_df['attack_success'].sum() / len(unsafe_df) * 100\n",
    "human_asr = unsafe_df['human_attack_success'].sum() / len(unsafe_df) * 100\n",
    "print(f\"   Judge ASR:  {judge_asr:.1f}% ({int(unsafe_df['attack_success'].sum())}/{len(unsafe_df)})\")\n",
    "print(f\"   Human ASR:  {human_asr:.1f}% ({int(unsafe_df['human_attack_success'].sum())}/{len(unsafe_df)})\")\n",
    "print(f\"   Difference: {human_asr - judge_asr:+.1f}%\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"REFUSAL RATE - All Prompts\")\n",
    "print(\"=\"*60)\n",
    "judge_ref = df['is_refusal'].sum() / len(df) * 100\n",
    "human_ref = df['human_refusal'].sum() / len(df) * 100\n",
    "print(f\"   Judge Refusal:  {judge_ref:.1f}% ({int(df['is_refusal'].sum())}/{len(df)})\")\n",
    "print(f\"   Human Refusal:  {human_ref:.1f}% ({int(df['human_refusal'].sum())}/{len(df)})\")\n",
    "print(f\"   Difference: {human_ref - judge_ref:+.1f}%\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CONFUSION MATRIX (Attack Success)\")\n",
    "print(\"=\"*60)\n",
    "tp = ((df['attack_success']==True) & (df['human_attack_success']==True)).sum()\n",
    "tn = ((df['attack_success']==False) & (df['human_attack_success']==False)).sum()\n",
    "fp = ((df['attack_success']==True) & (df['human_attack_success']==False)).sum()\n",
    "fn = ((df['attack_success']==False) & (df['human_attack_success']==True)).sum()\n",
    "prec = tp/(tp+fp)*100 if (tp+fp)>0 else 0\n",
    "rec = tp/(tp+fn)*100 if (tp+fn)>0 else 0\n",
    "f1 = 2*tp/(2*tp+fp+fn)*100 if (2*tp+fp+fn)>0 else 0\n",
    "print(f\"\"\"\n",
    "                     Human Says\n",
    "                   Attack | No Attack\n",
    "                 ---------+----------\n",
    "  Judge  Attack  |  {tp:3d}   |   {fp:3d}    ‚Üê FP (judge over-detected)\n",
    "      No Attack  |  {fn:3d}   |   {tn:3d}    ‚Üê FN (judge missed)\n",
    "\n",
    "  Precision: {prec:.1f}%  Recall: {rec:.1f}%  F1: {f1:.1f}%\n",
    "\"\"\")\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(\"BREAKDOWN BY DOMAIN (Unsafe Only)\")\n",
    "print(\"=\"*60)\n",
    "for domain in sorted(unsafe_df['domain'].unique()):\n",
    "    d_df = unsafe_df[unsafe_df['domain']==domain]\n",
    "    j = d_df['attack_success'].sum()/len(d_df)*100\n",
    "    h = d_df['human_attack_success'].sum()/len(d_df)*100\n",
    "    print(f\"   {domain:15s}: Judge={j:5.1f}%  Human={h:5.1f}%  Œî={h-j:+5.1f}%  (n={len(d_df)})\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"BREAKDOWN BY TIER (Unsafe Only)\")\n",
    "print(\"=\"*60)\n",
    "for tier in sorted(unsafe_df['difficulty_tier'].unique()):\n",
    "    t_df = unsafe_df[unsafe_df['difficulty_tier']==tier]\n",
    "    j = t_df['attack_success'].sum()/len(t_df)*100\n",
    "    h = t_df['human_attack_success'].sum()/len(t_df)*100\n",
    "    print(f\"   Tier {tier}: Judge={j:5.1f}%  Human={h:5.1f}%  Œî={h-j:+5.1f}%  (n={len(t_df)})\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"BREAKDOWN BY ATTACK METHOD (Unsafe Only)\")\n",
    "print(\"=\"*60)\n",
    "for method in unsafe_df['attack_method'].value_counts().index:\n",
    "    m_df = unsafe_df[unsafe_df['attack_method']==method]\n",
    "    j = m_df['attack_success'].sum()/len(m_df)*100\n",
    "    h = m_df['human_attack_success'].sum()/len(m_df)*100\n",
    "    print(f\"   {method:25s}: Judge={j:5.1f}%  Human={h:5.1f}%  Œî={h-j:+5.1f}%  (n={len(m_df)})\")\n",
    "\n",
    "df.to_csv(SAMPLE_FILE, index=False)\n",
    "print(f\"\\nüíæ Results saved with human-corrected labels!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
