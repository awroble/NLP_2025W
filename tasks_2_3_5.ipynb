{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tasks 2, 3, 5\n",
        "\n",
        "Extract parties, agreement types, and international organizations from Alabama agreements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForTokenClassification, pipeline\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllama_cpp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Llama\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m hf_hub_download\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1412\u001b[39m, in \u001b[36m_handle_fromlist\u001b[39m\u001b[34m(module, fromlist, import_, recursive)\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/utils/import_utils.py:2317\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2315\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module:\n\u001b[32m   2316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2317\u001b[39m         module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2318\u001b[39m         value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   2319\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/utils/import_utils.py:2345\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2343\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   2344\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2345\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2346\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2347\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/importlib/__init__.py:90\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     88\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     89\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:40\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     32\u001b[39m     cached_file,\n\u001b[32m     33\u001b[39m     extract_commit_hash,\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m     logging,\n\u001b[32m     38\u001b[39m )\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mencoder_decoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EncoderDecoderConfig\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauto_factory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _LazyAutoMapping\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfiguration_auto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     42\u001b[39m     CONFIG_MAPPING_NAMES,\n\u001b[32m     43\u001b[39m     AutoConfig,\n\u001b[32m   (...)\u001b[39m\u001b[32m     46\u001b[39m     replace_list_option_in_docstrings,\n\u001b[32m     47\u001b[39m )\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available():\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:43\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfiguration_auto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoConfig, model_type_to_module_name, replace_list_option_in_docstrings\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgeneration\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GenerationMixin\n\u001b[32m     46\u001b[39m logger = logging.get_logger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     48\u001b[39m _T = TypeVar(\u001b[33m\"\u001b[39m\u001b[33m_T\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1412\u001b[39m, in \u001b[36m_handle_fromlist\u001b[39m\u001b[34m(module, fromlist, import_, recursive)\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/utils/import_utils.py:2317\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2315\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module:\n\u001b[32m   2316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2317\u001b[39m         module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2318\u001b[39m         value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   2319\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/utils/import_utils.py:2345\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2343\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   2344\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2345\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2346\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2347\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/importlib/__init__.py:90\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     88\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     89\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/generation/utils.py:43\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mintegrations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdeepspeed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_deepspeed_zero3_enabled\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mintegrations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfsdp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_fsdp_managed_module\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmasking_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_masks_for_generate\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpytorch_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m isin_mps_friendly\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenization_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExtensionsTrie\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/masking_utils.py:40\u001b[39m\n\u001b[32m     37\u001b[39m _is_torch_xpu_available = is_torch_xpu_available()\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_torch_greater_or_equal_than_2_6:\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_trace_wrapped_higher_order_op\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TransformGetItemToIndex\n\u001b[32m     43\u001b[39m logger = logging.get_logger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mand_masks\u001b[39m(*mask_functions: Callable) -> Callable:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/_dynamo/__init__.py:13\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mTorchDynamo is a Python-level JIT compiler designed to make unmodified PyTorch programs faster.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mTorchDynamo hooks into the frame evaluation API in CPython (PEP 523) to dynamically modify Python\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m \u001b[33;03mseamlessly optimize PyTorch programs, including those using modern Python features.\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     14\u001b[39m     aot_compile,\n\u001b[32m     15\u001b[39m     config,\n\u001b[32m     16\u001b[39m     convert_frame,\n\u001b[32m     17\u001b[39m     eval_frame,\n\u001b[32m     18\u001b[39m     functional_export,\n\u001b[32m     19\u001b[39m     resume_execution,\n\u001b[32m     20\u001b[39m )\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackends\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mregistry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m list_backends, lookup_backend, register_backend\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcallback\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m callback_handler, on_compile_end, on_compile_start\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/_dynamo/aot_compile.py:15\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprecompile_context\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PrecompileContext\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m convert_frame\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhooks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Hooks\n\u001b[32m     19\u001b[39m log = logging.getLogger(\u001b[34m__name__\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py:57\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcallback\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CallbackTrigger\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_compile_pg\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msymbolic_convert\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorifyState\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_guards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compile_context, CompileContext, CompileId, tracing\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_logging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m structured\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py:59\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msymbolic_shapes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m guard_bool\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_functools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cache_method\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     60\u001b[39m     config,\n\u001b[32m     61\u001b[39m     exc,\n\u001b[32m     62\u001b[39m     graph_break_hints,\n\u001b[32m     63\u001b[39m     logging \u001b[38;5;28;01mas\u001b[39;00m torchdynamo_logging,\n\u001b[32m     64\u001b[39m     trace_rules,\n\u001b[32m     65\u001b[39m     variables,\n\u001b[32m     66\u001b[39m )\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbytecode_analysis\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     68\u001b[39m     get_indexof,\n\u001b[32m     69\u001b[39m     JUMP_OPNAMES,\n\u001b[32m     70\u001b[39m     livevars_analysis,\n\u001b[32m     71\u001b[39m     propagate_line_nums,\n\u001b[32m     72\u001b[39m )\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbytecode_transformation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     74\u001b[39m     cleaned_instructions,\n\u001b[32m     75\u001b[39m     create_binary_slice,\n\u001b[32m   (...)\u001b[39m\u001b[32m     87\u001b[39m     unique_id,\n\u001b[32m     88\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/_dynamo/trace_rules.py:58\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mresume_execution\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TORCH_DYNAMO_RESUME_IN_PREFIX\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     52\u001b[39m     getfile,\n\u001b[32m     53\u001b[39m     hashable,\n\u001b[32m   (...)\u001b[39m\u001b[32m     56\u001b[39m     unwrap_if_wrapper,\n\u001b[32m     57\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvariables\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     59\u001b[39m     BuiltinVariable,\n\u001b[32m     60\u001b[39m     FunctionalCallVariable,\n\u001b[32m     61\u001b[39m     FunctorchHigherOrderVariable,\n\u001b[32m     62\u001b[39m     LocalGeneratorFunctionVariable,\n\u001b[32m     63\u001b[39m     LocalGeneratorObjectVariable,\n\u001b[32m     64\u001b[39m     NestedUserFunctionVariable,\n\u001b[32m     65\u001b[39m     PolyfilledFunctionVariable,\n\u001b[32m     66\u001b[39m     ReparametrizeModuleCallVariable,\n\u001b[32m     67\u001b[39m     SkipFunctionVariable,\n\u001b[32m     68\u001b[39m     TorchInGraphFunctionVariable,\n\u001b[32m     69\u001b[39m     UserFunctionVariable,\n\u001b[32m     70\u001b[39m     UserMethodVariable,\n\u001b[32m     71\u001b[39m )\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvariables\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VariableTracker\n\u001b[32m     75\u001b[39m np: Optional[types.ModuleType] = \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/_dynamo/variables/__init__.py:19\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mThis package implements variable tracking and symbolic execution capabilities for Dynamo,\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mwhich are essential for converting Python code into FX graphs. It provides a comprehensive\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m \u001b[33;03mallows Dynamo to accurately trace and optimize Python code while preserving its semantics.\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VariableTracker\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbuiltin\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BuiltinVariable\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconstant\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConstantVariable, EnumVariable\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/_dynamo/variables/base.py:650\u001b[39m\n\u001b[32m    646\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    647\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mmap\u001b[39m(typestr, objs))\n\u001b[32m--> \u001b[39m\u001b[32m650\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m builder\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/_dynamo/variables/builder.py:94\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mguards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GuardBuilder, install_guard, make_dupe_guard\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpgo\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     88\u001b[39m     auto_dynamic,\n\u001b[32m     89\u001b[39m     auto_unset,\n\u001b[32m   (...)\u001b[39m\u001b[32m     92\u001b[39m     process_automatic_dynamic,\n\u001b[32m     93\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mside_effects\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SideEffects\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msource\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     96\u001b[39m     AttrProxySource,\n\u001b[32m     97\u001b[39m     AttrSource,\n\u001b[32m   (...)\u001b[39m\u001b[32m    123\u001b[39m     UnspecializedNNModuleSource,\n\u001b[32m    124\u001b[39m )\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    126\u001b[39m     _extract_tensor_dict,\n\u001b[32m    127\u001b[39m     build_checkpoint_variable,\n\u001b[32m   (...)\u001b[39m\u001b[32m    157\u001b[39m     wrap_fake_exception,\n\u001b[32m    158\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/_dynamo/side_effects.py:34\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Optional, TYPE_CHECKING\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvariables\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmisc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutogradFunctionContextVariable\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m graph_break_hints, utils, variables\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbytecode_transformation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     38\u001b[39m     bytecode_from_template,\n\u001b[32m     39\u001b[39m     create_call_function,\n\u001b[32m     40\u001b[39m     create_call_method,\n\u001b[32m     41\u001b[39m     create_instruction,\n\u001b[32m     42\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/_dynamo/variables/misc.py:61\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VariableTracker\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconstant\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConstantVariable\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NestedUserFunctionVariable, UserFunctionVariable\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01muser_defined\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m call_random_fn, is_standard_setattr, UserDefinedObjectVariable\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:88\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconstant\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConstantVariable\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfsdp\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_fully_shard\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _fsdp_param_group\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n\u001b[32m     90\u001b[39m     _fsdp_param_group = \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/distributed/fsdp/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_flat_param\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FlatParameter \u001b[38;5;28;01mas\u001b[39;00m FlatParameter\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_fully_shard\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      3\u001b[39m     CPUOffloadPolicy,\n\u001b[32m      4\u001b[39m     FSDPModule,\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m     UnshardHandle,\n\u001b[32m     10\u001b[39m )\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfully_sharded_data_parallel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     12\u001b[39m     BackwardPrefetch,\n\u001b[32m     13\u001b[39m     CPUOffload,\n\u001b[32m   (...)\u001b[39m\u001b[32m     27\u001b[39m     StateDictType,\n\u001b[32m     28\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/distributed/fsdp/_flat_param.py:33\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparameter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _ParameterMeta  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtesting\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_internal\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfake_pg\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FakeProcessGroup\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_fsdp_extensions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     34\u001b[39m     _ext_post_unflatten_transform,\n\u001b[32m     35\u001b[39m     _ext_pre_flatten_transform,\n\u001b[32m     36\u001b[39m     FSDPExtensions,\n\u001b[32m     37\u001b[39m )\n\u001b[32m     40\u001b[39m __all__ = [\n\u001b[32m     41\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mFlatParameter\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     42\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mFlatParamHandle\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     46\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mHandleShardingStrategy\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     47\u001b[39m ]\n\u001b[32m     49\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/distributed/fsdp/_fsdp_extensions.py:6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdist\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_shard\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msharded_tensor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ShardedTensor\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_shard\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msharded_tensor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mshard\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Shard\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfsdp\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_shard_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      9\u001b[39m     _all_gather_dtensor,\n\u001b[32m     10\u001b[39m     _create_chunk_dtensor,\n\u001b[32m     11\u001b[39m     _create_chunk_sharded_tensor,\n\u001b[32m     12\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/distributed/_shard/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _shard_tensor, load_with_process_group, shard_module, shard_parameter\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/distributed/_shard/api.py:9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distributed_c10d\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_shard\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msharded_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ShardedTensor\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01msharder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sharder\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01msharding_plan\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ShardingPlan\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/distributed/_shard/sharded_tensor/__init__.py:8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_shard\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mop_registry_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _decorator_func\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      9\u001b[39m     _CUSTOM_SHARDED_OPS,\n\u001b[32m     10\u001b[39m     _SHARDED_OPS,\n\u001b[32m     11\u001b[39m     Shard,\n\u001b[32m     12\u001b[39m     ShardedTensor,\n\u001b[32m     13\u001b[39m     ShardedTensorBase,\n\u001b[32m     14\u001b[39m     ShardedTensorMetadata,\n\u001b[32m     15\u001b[39m     TensorProperties,\n\u001b[32m     16\u001b[39m )\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetadata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ShardMetadata  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/distributed/_shard/sharded_tensor/api.py:16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdist\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_shard\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msharding_spec\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mshard_spec\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _get_device_module\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distributed_c10d, rpc\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/distributed/_shard/sharding_spec/__init__.py:10\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_shard\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetadata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ShardMetadata\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      4\u001b[39m     _infer_sharding_spec_from_shards_metadata,\n\u001b[32m      5\u001b[39m     DevicePlacementSpec,\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m     ShardingSpec,\n\u001b[32m      9\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchunk_sharding_spec\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChunkShardingSpec \u001b[38;5;28;01mas\u001b[39;00m ChunkShardingSpec\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/distributed/_shard/sharding_spec/chunk_sharding_spec.py:9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_shard\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msharded_tensor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetadata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msharded_tensor_meta\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed_c10d\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdistributed_c10d\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_shard\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m narrow_tensor\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_shard\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetadata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ShardMetadata\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_shard\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msharded_tensor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mshard\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Shard\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1331\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:935\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:991\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1087\u001b[39m, in \u001b[36mget_code\u001b[39m\u001b[34m(self, fullname)\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1187\u001b[39m, in \u001b[36mget_data\u001b[39m\u001b[34m(self, path)\u001b[39m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
        "from llama_cpp import Llama\n",
        "from huggingface_hub import hf_hub_download\n",
        "import glob\n",
        "import os\n",
        "import warnings\n",
        "import json\n",
        "import re\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import time\n",
        "import tracemalloc\n",
        "import resource\n",
        "import statistics\n",
        "import atexit\n",
        "\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mistral_model_name = \"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\"\n",
        "mistral_file = \"mistral-7b-instruct-v0.2.Q4_K_M.gguf\"\n",
        "mistral_path = hf_hub_download(repo_id=mistral_model_name, filename=mistral_file)\n",
        "llm = Llama(model_path=mistral_path, n_gpu_layers=-1, n_ctx=2048, verbose=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "classifier = pipeline(\n",
        "    \"zero-shot-classification\",\n",
        "    model=\"facebook/bart-large-mnli\",\n",
        "    device=0 if torch.cuda.is_available() else -1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name = \"dandoune/legal-NER\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "ner_model = AutoModelForTokenClassification.from_pretrained(model_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper functions\n",
        "\n",
        "- fetching all .txt files within one directory\n",
        "- saving results \n",
        "- time/memory profiling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_txt_files(directory):\n",
        "    path_pattern = os.path.join(directory, \"*.txt\")\n",
        "    return sorted(glob.glob(path_pattern))\n",
        "\n",
        "# Resolve paths robustly (works when running from repo root or from NLP_2025W)\n",
        "PROJECT_DIR = Path.cwd()\n",
        "if not (PROJECT_DIR / \"data\").exists():\n",
        "    PROJECT_DIR = Path(__file__).resolve().parent if \"__file__\" in globals() else Path.cwd()\n",
        "    if not (PROJECT_DIR / \"data\").exists():\n",
        "        PROJECT_DIR = Path.cwd().parent\n",
        "\n",
        "DATA_DIR = PROJECT_DIR / \"data\" / \"Alabama\"\n",
        "OUTPUT_DIR = PROJECT_DIR / \"outputs\" / \"tasks_2_3_5\"\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "RUN_ID = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "\n",
        "def _write_json(path: Path, obj):\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(obj, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "\n",
        "def _write_jsonl(path: Path, rows):\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        for r in rows:\n",
        "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "\n",
        "def _try_parse_json_from_llm(text: str):\n",
        "    if text is None:\n",
        "        return None\n",
        "    s = text.strip()\n",
        "    m = re.search(r\"\\{.*\\}\", s, flags=re.DOTALL)\n",
        "    if not m:\n",
        "        return None\n",
        "    candidate = m.group(0)\n",
        "    try:\n",
        "        return json.loads(candidate)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "def _max_rss_mb() -> float:\n",
        "    return float(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss) / 1024.0\n",
        "\n",
        "\n",
        "def _profile_start() -> dict:\n",
        "    tracemalloc.start()\n",
        "    return {\n",
        "        \"t0\": time.perf_counter(),\n",
        "        \"rss0_mb\": _max_rss_mb(),\n",
        "    }\n",
        "\n",
        "\n",
        "def _profile_end(start: dict) -> dict:\n",
        "    elapsed = time.perf_counter() - start[\"t0\"]\n",
        "    _, peak = tracemalloc.get_traced_memory()\n",
        "    tracemalloc.stop()\n",
        "    rss1 = _max_rss_mb()\n",
        "    return {\n",
        "        \"seconds\": float(elapsed),\n",
        "        \"py_peak_mb\": float(peak) / 1024.0 / 1024.0,\n",
        "        \"max_rss_mb\": float(rss1),\n",
        "        \"max_rss_delta_mb\": float(rss1 - start[\"rss0_mb\"]),\n",
        "    }\n",
        "\n",
        "\n",
        "def _summarize_times(times_s: list[float]) -> dict:\n",
        "    if not times_s:\n",
        "        return {\"count\": 0}\n",
        "    return {\n",
        "        \"count\": int(len(times_s)),\n",
        "        \"mean_s\": float(statistics.mean(times_s)),\n",
        "        \"median_s\": float(statistics.median(times_s)),\n",
        "        \"min_s\": float(min(times_s)),\n",
        "        \"max_s\": float(max(times_s)),\n",
        "    }\n",
        "\n",
        "\n",
        "PROFILE = {\n",
        "    \"run_id\": RUN_ID,\n",
        "    \"data_dir\": str(DATA_DIR),\n",
        "    \"output_dir\": str(OUTPUT_DIR),\n",
        "    \"methods\": {},\n",
        "}\n",
        "\n",
        "PROFILE_PATH = OUTPUT_DIR / f\"profiling_tasks_2_3_5_{RUN_ID}.json\"\n",
        "\n",
        "\n",
        "def _save_profile():\n",
        "    try:\n",
        "        _write_json(PROFILE_PATH, PROFILE)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "\n",
        "atexit.register(_save_profile)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 2 - extract agreements parties\n",
        "\n",
        "### 2.1 Legal NER for parties extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_entities(text, model, tokenizer, max_length=512):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=max_length)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        predictions = torch.argmax(outputs.logits, dim=2)[0].cpu().tolist()\n",
        "\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "\n",
        "    label_map = model.config.id2label\n",
        "    labels = [label_map.get(str(pred), label_map.get(pred, f\"LABEL_{pred}\")) for pred in predictions]\n",
        "\n",
        "    entities = []\n",
        "    current_tokens = []\n",
        "    current_label = None\n",
        "    print({token: label for token, label in zip(tokens, labels)})\n",
        "    for token, label in zip(tokens, labels):\n",
        "        if token in ['[CLS]', '[SEP]', '[PAD]']:\n",
        "            if current_tokens and current_label and current_label != 'LABEL_8' and current_label != 'O':\n",
        "                entity_text = tokenizer.convert_tokens_to_string(current_tokens)\n",
        "                if entity_text.strip() and not entity_text.strip().startswith('##'):\n",
        "                    entities.append((entity_text.strip(), current_label))\n",
        "            current_tokens = []\n",
        "            current_label = None\n",
        "            continue\n",
        "\n",
        "        if label == 'LABEL_8' or label == 'O':\n",
        "            if current_tokens and current_label:\n",
        "                entity_text = tokenizer.convert_tokens_to_string(current_tokens)\n",
        "                if entity_text.strip() and not entity_text.strip().startswith('##'):\n",
        "                    entities.append((entity_text.strip(), current_label))\n",
        "            current_tokens = []\n",
        "            current_label = None\n",
        "        elif label == current_label:\n",
        "            current_tokens.append(token)\n",
        "        else:\n",
        "            if current_tokens and current_label:\n",
        "                entity_text = tokenizer.convert_tokens_to_string(current_tokens)\n",
        "                if entity_text.strip() and not entity_text.strip().startswith('##'):\n",
        "                    entities.append((entity_text.strip(), current_label))\n",
        "            current_tokens = [token]\n",
        "            current_label = label\n",
        "\n",
        "    if current_tokens and current_label and current_label != 'LABEL_8' and current_label != 'O':\n",
        "        entity_text = tokenizer.convert_tokens_to_string(current_tokens)\n",
        "        if entity_text.strip() and not entity_text.strip().startswith('##'):\n",
        "            entities.append((entity_text.strip(), current_label))\n",
        "\n",
        "    return entities\n",
        "\n",
        "_profile = _profile_start()\n",
        "with open(\"/home/ubuntu/nlp/data/Alabama/Alabama_3.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    alabama_text = f.read()\n",
        "\n",
        "ner_entities = extract_entities(alabama_text, ner_model, tokenizer)\n",
        "profile_stats = _profile_end(_profile)\n",
        "\n",
        "PROFILE[\"methods\"][\"task2_ner_single\"] = {\n",
        "    \"input\": {\"filename\": \"Alabama_3.txt\", \"chars\": len(alabama_text)},\n",
        "    \"performance\": profile_stats,\n",
        "    \"output\": {\"n_entities\": len(ner_entities)},\n",
        "}\n",
        "_save_profile()\n",
        "\n",
        "_write_json(\n",
        "    OUTPUT_DIR / f\"task2_parties_ner_single_{RUN_ID}.json\",\n",
        "    {\n",
        "        \"filename\": \"Alabama_3.txt\",\n",
        "        \"entities\": [{\"text\": t, \"label\": l} for t, l in ner_entities],\n",
        "    },\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "entities_by_label = defaultdict(list)\n",
        "for entity_text, label in ner_entities:\n",
        "    if len(entity_text.strip()) > 2:\n",
        "        entities_by_label[label].append(entity_text.strip())\n",
        "\n",
        "print(f\"Total entities found: {len(ner_entities)}\\n\")\n",
        "for label in sorted(entities_by_label.keys()):\n",
        "    unique_entities = sorted(set(entities_by_label[label]))\n",
        "    print(f\"\\n{label}:\")\n",
        "    for entity in unique_entities[:10]:\n",
        "        print(f\"{entity}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Parties extraction results for legal NER model\n",
        "\n",
        "Several NER models have been tested but the task seems to be too complex for this approach. Agreement parties have often very long names and contain other country/institution/person name in it. This leads to not recognizing the full name of the agreement party. \n",
        "\n",
        "NER models finetuned for legal purposes were hard to find though - often publicly available sources were broken. More trusted models could could turn out better.\n",
        "\n",
        "### 2.2 Prompting Mistral model for agreement parties"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_parties(text_chunk):\n",
        "    prompt = f\"\"\"[INST] You are a legal AI. Extract the contracting parties from the text below and only the direct agreement parties.\n",
        "    Return ONLY a JSON object with keys: \"parties\" (list of objects with \"name\", \"role\").\n",
        "    \n",
        "    TEXT:\n",
        "    {text_chunk}\n",
        "    [/INST]\"\"\"\n",
        "    output = llm(prompt, max_tokens=300, temperature=0.1, stop=[\"[/INST]\"])\n",
        "    return output['choices'][0]['text']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "txt_files = find_txt_files(str(DATA_DIR))\n",
        "parties_results = []\n",
        "per_doc_times = []\n",
        "\n",
        "_profile = _profile_start()\n",
        "for txt_file in txt_files:\n",
        "    with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read(1500)\n",
        "    t0 = time.perf_counter()\n",
        "    result = extract_parties(text)\n",
        "    per_doc_times.append(time.perf_counter() - t0)\n",
        "    parties_results.append({\n",
        "        'filename': os.path.basename(txt_file),\n",
        "        'parties': result\n",
        "    })\n",
        "profile_stats = _profile_end(_profile)\n",
        "\n",
        "parties_results_serialized = []\n",
        "for r in parties_results:\n",
        "    parties_results_serialized.append({\n",
        "        **r,\n",
        "        \"parsed\": _try_parse_json_from_llm(r.get(\"parties\")),\n",
        "    })\n",
        "\n",
        "PROFILE[\"methods\"][\"task2_parties_mistral\"] = {\n",
        "    \"input\": {\"n_files\": len(txt_files), \"chars_per_file\": 1500},\n",
        "    \"performance\": {\n",
        "        **profile_stats,\n",
        "        \"per_doc\": _summarize_times(per_doc_times),\n",
        "    },\n",
        "    \"output\": {\n",
        "        \"n_rows\": len(parties_results_serialized),\n",
        "        \"parse_ok\": int(sum(1 for r in parties_results_serialized if isinstance(r.get(\"parsed\"), dict))),\n",
        "    },\n",
        "}\n",
        "_save_profile()\n",
        "\n",
        "_write_json(OUTPUT_DIR / f\"task2_parties_mistral_{RUN_ID}.json\", parties_results_serialized)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Parties extraction results for Mistral model\n",
        "\n",
        "The following results show all contracting parties extracted from each agreement using the Mistral 7B model specifically fine-tuned to follow user instructions, and generate structured text. Approprietly prompted model returned correct parties of the agreement. In more complex scenarios errors occured - e.g. canadian provinces/USA states listed as members of one party were treated as separate agreement party. Additional \"role\" field explains the entity role in the agreement.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for result in parties_results:\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"File: {result['filename']}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(result['parties'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 3 - agreement type classification\n",
        "\n",
        "### 3.1 Zero shot classification with BART model\n",
        "\n",
        "Some of the most popular international/interstate agreements type were given as classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "agreement_types = [\n",
        "    \"Economic Cooperation Agreement\",\n",
        "    \"Trade Agreement\",\n",
        "    \"Investment Agreement\",\n",
        "    \"Memorandum of Understanding (MOU)\",\n",
        "    \"Partnership Agreement\",\n",
        "    \"Educational Exchange Agreement\",\n",
        "    \"Cultural Exchange Agreement\",\n",
        "    \"Technology Transfer Agreement\",\n",
        "    \"Waterway Development Agreement\",\n",
        "    \"Interstate Compact\",\n",
        "    \"Sister State Agreement\",\n",
        "    \"Sister City Agreement\",\n",
        "    \"Development Cooperation Agreement\"\n",
        "]\n",
        "\n",
        "def classify_agreement_type(text, max_length=512):\n",
        "    if len(text) > max_length * 4:\n",
        "        header = text[:max_length * 2]\n",
        "        footer = text[-max_length * 2:]\n",
        "        text_to_classify = header + \"\\n\\n\" + footer\n",
        "    else:\n",
        "        text_to_classify = text\n",
        "    return classifier(text_to_classify, agreement_types, multi_label=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "classification_results = []\n",
        "txt_files = find_txt_files(str(DATA_DIR))\n",
        "per_doc_times = []\n",
        "\n",
        "_profile = _profile_start()\n",
        "for txt_file in txt_files:\n",
        "    with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read()\n",
        "    t0 = time.perf_counter()\n",
        "    result = classify_agreement_type(text)\n",
        "    per_doc_times.append(time.perf_counter() - t0)\n",
        "    classification_results.append({\n",
        "        'filename': os.path.basename(txt_file),\n",
        "        'predicted_type': result['labels'][0],\n",
        "        'confidence': result['scores'][0],\n",
        "        'top_3': list(zip(result['labels'][:3], result['scores'][:3]))\n",
        "    })\n",
        "profile_stats = _profile_end(_profile)\n",
        "\n",
        "PROFILE[\"methods\"][\"task3_type_bart\"] = {\n",
        "    \"input\": {\"n_files\": len(txt_files), \"chars_per_file\": None},\n",
        "    \"performance\": {\n",
        "        **profile_stats,\n",
        "        \"per_doc\": _summarize_times(per_doc_times),\n",
        "    },\n",
        "    \"output\": {\"n_rows\": len(classification_results)},\n",
        "}\n",
        "_save_profile()\n",
        "\n",
        "_write_json(OUTPUT_DIR / f\"task3_type_bart_{RUN_ID}.json\", classification_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Agreement Type Classification Results\n",
        "\n",
        "The following results show the agreement type classification for each document using zero-shot classification. An expert knowledge is needed to verify the classification because some agreements do not have the goal clearly listed. There are several Memorandas of Understanding though and they were recognized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "agreement_type_counts = Counter([r['predicted_type'] for r in classification_results])\n",
        "print(f\"Total files classified: {len(classification_results)}\\n\")\n",
        "print(\"Agreement type distribution:\")\n",
        "print(\"-\" * 80)\n",
        "for agreement_type, count in agreement_type_counts.most_common():\n",
        "    print(f\"  {agreement_type:50} : {count:2} files\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Detailed Results:\")\n",
        "print(\"=\" * 80)\n",
        "for result in classification_results:\n",
        "    print(f\"\\n{result['filename']}\")\n",
        "    print(f\"  Type: {result['predicted_type']}\")\n",
        "    print(f\"  Confidence: {result['confidence']:.3f}\")\n",
        "    print(f\"  Top 3 predictions:\")\n",
        "    for label, score in result['top_3']:\n",
        "        print(f\"    - {label}: {score:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Prompting Mistral model for agreement type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_agreement_type_mistral(text_chunk):\n",
        "    prompt = f\"\"\"[INST] You are a legal AI. Analyze the legal agreement text below and identify type of agreement. Return only one type of agreement.\n",
        "    Return ONLY a JSON object with keys: \"agreement_type\" (the specific type of agreement, e.g., \"Economic Cooperation Agreement\", \"Trade Agreement\", \"Memorandum of Understanding\", etc.) and \"description\" (a brief description of what the agreement is about).\n",
        "    \n",
        "    TEXT:\n",
        "    {text_chunk}\n",
        "    [/INST]\"\"\"\n",
        "    output = llm(prompt, max_tokens=200, temperature=0.1, stop=[\"[/INST]\"])\n",
        "    return output['choices'][0]['text']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "agreement_type_results = []\n",
        "txt_files = find_txt_files(str(DATA_DIR))\n",
        "per_doc_times = []\n",
        "\n",
        "_profile = _profile_start()\n",
        "for txt_file in txt_files:\n",
        "    with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read(2000)\n",
        "    t0 = time.perf_counter()\n",
        "    result = extract_agreement_type_mistral(text)\n",
        "    per_doc_times.append(time.perf_counter() - t0)\n",
        "    agreement_type_results.append({\n",
        "        'filename': os.path.basename(txt_file),\n",
        "        'extraction': result\n",
        "    })\n",
        "profile_stats = _profile_end(_profile)\n",
        "\n",
        "agreement_type_results_serialized = []\n",
        "for r in agreement_type_results:\n",
        "    agreement_type_results_serialized.append({\n",
        "        **r,\n",
        "        \"parsed\": _try_parse_json_from_llm(r.get(\"extraction\")),\n",
        "    })\n",
        "\n",
        "PROFILE[\"methods\"][\"task3_type_mistral\"] = {\n",
        "    \"input\": {\"n_files\": len(txt_files), \"chars_per_file\": 2000},\n",
        "    \"performance\": {\n",
        "        **profile_stats,\n",
        "        \"per_doc\": _summarize_times(per_doc_times),\n",
        "    },\n",
        "    \"output\": {\n",
        "        \"n_rows\": len(agreement_type_results_serialized),\n",
        "        \"parse_ok\": int(sum(1 for r in agreement_type_results_serialized if isinstance(r.get(\"parsed\"), dict))),\n",
        "    },\n",
        "}\n",
        "_save_profile()\n",
        "\n",
        "_write_json(OUTPUT_DIR / f\"task3_type_mistral_{RUN_ID}.json\", agreement_type_results_serialized)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Agreement type extraction results (Mistral)\n",
        "\n",
        "The following results show the agreement types extracted directly from the text. Unlike classification, this approach allows  to identify agreement types without predefined categories. Surprisingly - almost all of the agreements have been recognized as Memoranda of Understanding that was actually present in the class subset for classification. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Total files processed: {len(agreement_type_results)}\\n\")\n",
        "print(\"=\" * 80)\n",
        "for result in agreement_type_results:\n",
        "    print(f\"\\n{result['filename']}\")\n",
        "    print(\"-\" * 80)\n",
        "    print(result['extraction'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 5 - Extracting international organizations\n",
        "\n",
        "This task was supposed to be solved based on NER results but they turned out to be poor just like in Task 2. Mistral model has been utilized again to find all such entities. Model has been instructed to ignore domestic/local entities and explain context in which the organization appears in the agreement in the \"type\" field."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_international_orgs(text_chunk):\n",
        "    prompt = f\"\"\"[INST] You are a legal AI. From the legal agreement text below, extract all INTERNATIONAL ORGANIZATIONS mentioned.\n",
        "    \n",
        "    - Focus on organizations that operate across national borders (e.g., international organizations, intergovernmental bodies, multinational commissions).\n",
        "    - Ignore purely national ministries, state agencies, or local entities unless they are clearly part of an international body.\n",
        "    - Return ONLY a JSON object with the key \"international_organizations\" which is a list of objects with fields:\n",
        "        - \"name\": the full official name as it appears or can be reasonably completed\n",
        "        - \"type\": short description (e.g., \"international organization\", \"intergovernmental commission\", \"binational authority\").\n",
        "    \n",
        "    TEXT:\n",
        "    {text_chunk}\n",
        "    [/INST]\"\"\"\n",
        "    output = llm(prompt, max_tokens=300, temperature=0.1, stop=[\"[/INST]\"])\n",
        "    return output['choices'][0]['text']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "intl_orgs_results = []\n",
        "txt_files = find_txt_files(str(DATA_DIR))\n",
        "per_doc_times = []\n",
        "\n",
        "_profile = _profile_start()\n",
        "for txt_file in txt_files:\n",
        "    with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read()\n",
        "    t0 = time.perf_counter()\n",
        "    result = extract_international_orgs(text[:2000])\n",
        "    per_doc_times.append(time.perf_counter() - t0)\n",
        "    intl_orgs_results.append({\n",
        "        'filename': os.path.basename(txt_file),\n",
        "        'extraction': result\n",
        "    })\n",
        "profile_stats = _profile_end(_profile)\n",
        "\n",
        "intl_orgs_results_serialized = []\n",
        "for r in intl_orgs_results:\n",
        "    intl_orgs_results_serialized.append({\n",
        "        **r,\n",
        "        \"parsed\": _try_parse_json_from_llm(r.get(\"extraction\")),\n",
        "    })\n",
        "\n",
        "PROFILE[\"methods\"][\"task5_orgs_mistral\"] = {\n",
        "    \"input\": {\"n_files\": len(txt_files), \"chars_per_file\": 2000},\n",
        "    \"performance\": {\n",
        "        **profile_stats,\n",
        "        \"per_doc\": _summarize_times(per_doc_times),\n",
        "    },\n",
        "    \"output\": {\n",
        "        \"n_rows\": len(intl_orgs_results_serialized),\n",
        "        \"parse_ok\": int(sum(1 for r in intl_orgs_results_serialized if isinstance(r.get(\"parsed\"), dict))),\n",
        "    },\n",
        "}\n",
        "_save_profile()\n",
        "\n",
        "_write_json(OUTPUT_DIR / f\"task5_international_orgs_mistral_{RUN_ID}.json\", intl_orgs_results_serialized)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "intl_orgs_results_serialized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### International organizations extraction results\n",
        "\n",
        "The following results show all international or supranational organizations mentioned in each agreement. These are organizations that operate across national borders, such as intergovernmental bodies, multinational commissions, or international organizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save profiling summary\n",
        "_save_profile()\n",
        "\n",
        "# Print a compact profiling table\n",
        "rows = []\n",
        "for name, rec in PROFILE.get(\"methods\", {}).items():\n",
        "    perf = rec.get(\"performance\", {})\n",
        "    per_doc = perf.get(\"per_doc\", {}) if isinstance(perf, dict) else {}\n",
        "    rows.append({\n",
        "        \"method\": name,\n",
        "        \"total_s\": perf.get(\"seconds\"),\n",
        "        \"per_doc_mean_s\": per_doc.get(\"mean_s\"),\n",
        "        \"per_doc_median_s\": per_doc.get(\"median_s\"),\n",
        "        \"py_peak_mb\": perf.get(\"py_peak_mb\"),\n",
        "        \"max_rss_mb\": perf.get(\"max_rss_mb\"),\n",
        "    })\n",
        "\n",
        "try:\n",
        "    import pandas as pd\n",
        "\n",
        "    df_profile = pd.DataFrame(rows).sort_values(\"total_s\", ascending=False)\n",
        "    display(df_profile)\n",
        "    print(f\"Profiling saved to: {PROFILE_PATH}\")\n",
        "except Exception:\n",
        "    print(rows)\n",
        "    print(f\"Profiling saved to: {PROFILE_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for result in intl_orgs_results:\n",
        "    print(f\"\\n{result['filename']}\")\n",
        "    print(\"-\" * 80)\n",
        "    print(result['extraction'])\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
