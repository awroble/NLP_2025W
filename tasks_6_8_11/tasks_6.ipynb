{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c945cd0",
   "metadata": {},
   "source": [
    "# HeidelTime Standalone + TreeTagger — Complete Ablation Notebook\n",
    "\n",
    "This notebook runs **HeidelTime Standalone** on your OCR `.txt` files in two scenarios:\n",
    "\n",
    "1. **Without POS tagger** (`-pos NO`) — no TreeTagger required\n",
    "2. **With TreeTagger POS** (`-pos TREETAGGER`) — requires TreeTagger + `english.par`\n",
    "\n",
    "It then **compares** the results and saves CSVs for your ablation study.\n",
    "\n",
    "✅ Robust parsing: HeidelTime output sometimes includes logs mixed with XML; we extract only the `<TimeML>...</TimeML>` block before parsing.\n",
    "\n",
    "Expected repo layout:\n",
    "- `heideltime-standalone/de.unihd.dbs.heideltime.standalone.jar`\n",
    "- `heideltime-standalone/config.props`\n",
    "- `OCR_output/<STATE>/*.txt`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba28f74",
   "metadata": {},
   "source": [
    "## 0) Prerequisites\n",
    "\n",
    "### Java\n",
    "HeidelTime is a Java program. `java -version` must work.\n",
    "\n",
    "### TreeTagger (only for POS scenario)\n",
    "You need:\n",
    "- `D:\\\\TreeTagger\\\\bin\\\\tree-tagger.exe`\n",
    "- `D:\\\\TreeTagger\\\\lib\\\\english.par` (download **English parameter file (PENN tagset)** and extract `english.par.gz`)\n",
    "\n",
    "If you don't have `english.par`, TreeTagger mode will fail.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5428e7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: d:\\NLP_Project_tasks_6_8_11\n",
      "HT_DIR      : d:\\NLP_Project_tasks_6_8_11\\heideltime-standalone\n",
      "HT_JAR      : d:\\NLP_Project_tasks_6_8_11\\heideltime-standalone\\de.unihd.dbs.heideltime.standalone.jar exists: True\n",
      "HT_CONF     : d:\\NLP_Project_tasks_6_8_11\\heideltime-standalone\\config.props exists: True\n",
      "OCR_DIR     : d:\\NLP_Project_tasks_6_8_11\\OCR_output exists: True\n",
      "STATE_DIR   : d:\\NLP_Project_tasks_6_8_11\\OCR_output\\California exists: True\n",
      "TREETAGGER_HOME: D:\\NLP_Project_tasks_6_8_11\\TreeTagger\n"
     ]
    }
   ],
   "source": [
    "import os, subprocess, shutil, re\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =====================\n",
    "# 1) Paths (edit only if needed)\n",
    "# =====================\n",
    "# Recommended: run this notebook from the REPO ROOT that contains `heideltime-standalone/` and `OCR_output/`\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "\n",
    "HT_DIR  = PROJECT_ROOT / \"heideltime-standalone\"\n",
    "HT_JAR  = HT_DIR / \"de.unihd.dbs.heideltime.standalone.jar\"\n",
    "HT_CONF = HT_DIR / \"config.props\"\n",
    "\n",
    "OCR_DIR = PROJECT_ROOT / \"OCR_output\"\n",
    "\n",
    "# =====================\n",
    "# 2) Choose what to run\n",
    "# =====================\n",
    "STATE = \"California\"  # <- change if needed\n",
    "STATE_DIR = OCR_DIR / STATE\n",
    "\n",
    "# =====================\n",
    "# 3) HeidelTime parameters\n",
    "# =====================\n",
    "LANG    = \"ENGLISH\"\n",
    "DOCTYPE = \"NARRATIVES\"     # agreements are usually fine as NARRATIVES\n",
    "OUTPUT  = \"TIMEML\"         # we parse TIMEML\n",
    "DCT     = datetime.date.today().strftime(\"%Y-%m-%d\")  # used only for DOCTYPE NEWS/COLLOQUIAL\n",
    "\n",
    "# =====================\n",
    "# 4) TreeTagger location (set this!)\n",
    "# =====================\n",
    "# Set to your TreeTagger root folder; set to None to skip TreeTagger run\n",
    "TREETAGGER_HOME = Path(r\"D:\\NLP_Project_tasks_6_8_11\\TreeTagger\")\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"HT_DIR      :\", HT_DIR)\n",
    "print(\"HT_JAR      :\", HT_JAR, \"exists:\", HT_JAR.exists())\n",
    "print(\"HT_CONF     :\", HT_CONF, \"exists:\", HT_CONF.exists())\n",
    "print(\"OCR_DIR     :\", OCR_DIR, \"exists:\", OCR_DIR.exists())\n",
    "print(\"STATE_DIR   :\", STATE_DIR, \"exists:\", STATE_DIR.exists())\n",
    "print(\"TREETAGGER_HOME:\", TREETAGGER_HOME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743905b6",
   "metadata": {},
   "source": [
    "## 1) Environment checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d34360d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Java OK: True\n",
      "openjdk version \"25.0.1\" 2025-10-21 LTS\n",
      "OpenJDK Runtime Environment Temurin-25.0.1+8 (build 25.0.1+8-LTS)\n",
      "OpenJDK 64-Bit Server VM Temurin-25.0.1+8 (build 25.0.1+8-LTS, mixed mode, sharing)\n",
      "TXT files found: 116\n",
      "Example: [WindowsPath('d:/NLP_Project_tasks_6_8_11/OCR_output/California/1 (5).txt'), WindowsPath('d:/NLP_Project_tasks_6_8_11/OCR_output/California/1 (CA - Chille).txt'), WindowsPath('d:/NLP_Project_tasks_6_8_11/OCR_output/California/110242023.txt')]\n"
     ]
    }
   ],
   "source": [
    "def check(cmd):\n",
    "    try:\n",
    "        out = subprocess.check_output(cmd, stderr=subprocess.STDOUT, text=True)\n",
    "        return True, out.strip()\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "ok_java, java_out = check([\"java\", \"-version\"])\n",
    "print(\"Java OK:\", ok_java)\n",
    "print(java_out[:800])\n",
    "\n",
    "if not ok_java:\n",
    "    raise RuntimeError(\n",
    "        \"Java not found. Install JDK (e.g., Temurin 17) and ensure `java -version` works in a NEW terminal / after restarting VS Code.\"\n",
    "    )\n",
    "\n",
    "if not HT_JAR.exists():\n",
    "    raise FileNotFoundError(f\"Missing HeidelTime JAR at: {HT_JAR}\")\n",
    "if not HT_CONF.exists():\n",
    "    raise FileNotFoundError(f\"Missing HeidelTime config.props at: {HT_CONF}\")\n",
    "if not STATE_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Missing OCR state folder at: {STATE_DIR}\")\n",
    "\n",
    "txt_files = sorted(STATE_DIR.glob(\"*.txt\"))\n",
    "print(\"TXT files found:\", len(txt_files))\n",
    "print(\"Example:\", txt_files[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2380cedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree-tagger.exe        -> D:\\TreeTagger\\bin\\tree-tagger.exe  exists=True\n",
      "english.par            -> D:\\TreeTagger\\lib\\english.par  exists=True\n",
      "english-abbreviations  -> D:\\TreeTagger\\lib\\english-abbreviations  exists=True\n",
      "utf8-tokenize.perl     -> D:\\TreeTagger\\cmd\\utf8-tokenize.perl  exists=True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "tt = Path(\"D:/TreeTagger\")\n",
    "checks = {\n",
    "    \"tree-tagger.exe\": tt/\"bin\"/\"tree-tagger.exe\",\n",
    "    \"english.par\": tt/\"lib\"/\"english.par\",\n",
    "    \"english-abbreviations\": tt/\"lib\"/\"english-abbreviations\",\n",
    "    \"utf8-tokenize.perl\": tt/\"cmd\"/\"utf8-tokenize.perl\",\n",
    "}\n",
    "\n",
    "for k,v in checks.items():\n",
    "    print(f\"{k:22} -> {v}  exists={v.exists()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b63b8dd",
   "metadata": {},
   "source": [
    "## 2) TreeTagger validation + patching config.props\n",
    "HeidelTime reads TreeTagger location from `config.props` (commonly `treeTaggerHome=...`).\n",
    "We create a **patched copy** of the config for the TreeTagger run, leaving your original untouched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23cc31a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TreeTagger OK:\n",
      "  bin: D:\\NLP_Project_tasks_6_8_11\\TreeTagger\\bin\n",
      "  lib: D:\\NLP_Project_tasks_6_8_11\\TreeTagger\\lib\n",
      "  english.par: D:\\NLP_Project_tasks_6_8_11\\TreeTagger\\lib\\english.par\n",
      "Patched config written to: d:\\NLP_Project_tasks_6_8_11\\heideltime-standalone\\config.props.patched\n",
      "treeTaggerHome line: treeTaggerHome=D:\\\\NLP_Project_tasks_6_8_11\\\\TreeTagger\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:28: SyntaxWarning: invalid escape sequence '\\T'\n",
      "<>:28: SyntaxWarning: invalid escape sequence '\\T'\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_29260\\4075816753.py:28: SyntaxWarning: invalid escape sequence '\\T'\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "def validate_treetagger_install(treetagger_home: Path) -> None:\n",
    "    if treetagger_home is None:\n",
    "        return\n",
    "    if not treetagger_home.exists():\n",
    "        raise FileNotFoundError(f\"TREETAGGER_HOME does not exist: {treetagger_home}\")\n",
    "\n",
    "    bin_dir = treetagger_home / \"bin\"\n",
    "    lib_dir = treetagger_home / \"lib\"\n",
    "\n",
    "    exe1 = bin_dir / \"tree-tagger.exe\"\n",
    "    exe2 = bin_dir / \"tree-tagger\"\n",
    "    if not (exe1.exists() or exe2.exists()):\n",
    "        raise FileNotFoundError(f\"TreeTagger binary not found in: {bin_dir}\")\n",
    "\n",
    "    eng_par = lib_dir / \"english.par\"\n",
    "    if not eng_par.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Missing TreeTagger English parameter file: {eng_par}\\n\"\n",
    "            \"Download 'English parameter file (PENN tagset)' (english.par.gz), extract to english.par, and place it in lib/.\"\n",
    "        )\n",
    "\n",
    "    print(\"TreeTagger OK:\")\n",
    "    print(\"  bin:\", bin_dir)\n",
    "    print(\"  lib:\", lib_dir)\n",
    "    print(\"  english.par:\", eng_par)\n",
    "    \n",
    "def patch_config_for_treetagger(conf_path: Path, treetagger_home: Path, key: str = \"treeTaggerHome\") -> Path:\n",
    "    \"\"\"\n",
    "    Write a patched config.props with treeTaggerHome set.\n",
    "\n",
    "    IMPORTANT (Windows): Java .properties treats backslash as an escape.\n",
    "    If we write `D:\\TreeTagger`, Java reads it as `D:TreeTagger`.\n",
    "    So we must escape backslashes -> `D:\\\\TreeTagger` (double backslash in file).\n",
    "    \"\"\"\n",
    "    text = conf_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    lines = text.splitlines()\n",
    "\n",
    "    # Escape backslashes for Java properties parsing\n",
    "    tt_home = str(treetagger_home).replace('\\\\', '\\\\\\\\')\n",
    "\n",
    "    found = False\n",
    "    new_lines = []\n",
    "    for line in lines:\n",
    "        if line.strip().startswith(f\"{key}=\"):\n",
    "            new_lines.append(f\"{key}={tt_home}\")\n",
    "            found = True\n",
    "        else:\n",
    "            new_lines.append(line)\n",
    "\n",
    "    if not found:\n",
    "        new_lines.append(f\"{key}={tt_home}\")\n",
    "\n",
    "    patched = conf_path.with_suffix(conf_path.suffix + \".patched\")\n",
    "    patched.write_text('\\n'.join(new_lines), encoding=\"utf-8\")\n",
    "    return patched\n",
    "\n",
    "\n",
    "# ---- RUN THE PATCH (this creates `patched_conf`) ----\n",
    "\n",
    "patched_conf = None\n",
    "\n",
    "if TREETAGGER_HOME is None:\n",
    "    raise RuntimeError(\n",
    "        \"TREETAGGER_HOME is not set.\\n\"\n",
    "        \"Set TREETAGGER_HOME to your TreeTagger folder (the one that contains bin/ and lib/), \"\n",
    "        \"then re-run this notebook from Section 2.\\n\"\n",
    "        \"Example (Linux/macOS): export TREETAGGER_HOME=~/TreeTagger\\n\"\n",
    "        r\"Example (Windows PowerShell): $env:TREETAGGER_HOME='C:\\TreeTagger'\\n\"\n",
    "    )\n",
    "\n",
    "validate_treetagger_install(TREETAGGER_HOME)\n",
    "patched_conf = patch_config_for_treetagger(\n",
    "    HT_CONF,\n",
    "    TREETAGGER_HOME,\n",
    "    key=\"treeTaggerHome\"\n",
    ")\n",
    "print(\"Patched config written to:\", patched_conf)\n",
    "print(\"treeTaggerHome line:\", [l for l in patched_conf.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines() if l.startswith(\"treeTaggerHome=\")][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a139e7e",
   "metadata": {},
   "source": [
    "## 3) Run HeidelTime on one file (smoke test)\n",
    "Includes robust `<TimeML>` extraction to avoid XML parse errors from mixed stdout/logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73605d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example file: d:\\NLP_Project_tasks_6_8_11\\OCR_output\\California\\1 (5).txt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tid</th>\n",
       "      <th>type</th>\n",
       "      <th>value</th>\n",
       "      <th>mod</th>\n",
       "      <th>quant</th>\n",
       "      <th>freq</th>\n",
       "      <th>beginPoint</th>\n",
       "      <th>endPoint</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3</td>\n",
       "      <td>DATE</td>\n",
       "      <td>19</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>twentieth\\ncentury</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t4</td>\n",
       "      <td>DATE</td>\n",
       "      <td>1961</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t17</td>\n",
       "      <td>DURATION</td>\n",
       "      <td>P1Y</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>twelve months</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t16</td>\n",
       "      <td>DATE</td>\n",
       "      <td>1961-06-30</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>June 30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t28</td>\n",
       "      <td>DATE</td>\n",
       "      <td>PRESENT_REF</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>current</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>t29</td>\n",
       "      <td>DATE</td>\n",
       "      <td>1957</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>t30</td>\n",
       "      <td>DATE</td>\n",
       "      <td>1956</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>t32</td>\n",
       "      <td>DURATION</td>\n",
       "      <td>P30D</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>30 days</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tid      type        value   mod quant  freq beginPoint endPoint  \\\n",
       "0   t3      DATE           19  None  None  None       None     None   \n",
       "1   t4      DATE         1961  None  None  None       None     None   \n",
       "2  t17  DURATION          P1Y  None  None  None       None     None   \n",
       "3  t16      DATE   1961-06-30  None  None  None       None     None   \n",
       "4  t28      DATE  PRESENT_REF  None  None  None       None     None   \n",
       "5  t29      DATE         1957  None  None  None       None     None   \n",
       "6  t30      DATE         1956  None  None  None       None     None   \n",
       "7  t32  DURATION         P30D  None  None  None       None     None   \n",
       "\n",
       "                 text  \n",
       "0  twentieth\\ncentury  \n",
       "1                1961  \n",
       "2       twelve months  \n",
       "3             June 30  \n",
       "4             current  \n",
       "5                1957  \n",
       "6                1956  \n",
       "7             30 days  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_heideltime(txt_path: Path, pos_mode: str, conf_path: Path) -> str:\n",
    "    cmd = [\n",
    "        \"java\", \"-Dfile.encoding=UTF-8\",\n",
    "        \"-jar\", str(HT_JAR),\n",
    "        str(txt_path),\n",
    "        \"-c\", str(conf_path),\n",
    "        \"-l\", LANG,\n",
    "        \"-t\", DOCTYPE,\n",
    "        \"-o\", OUTPUT,\n",
    "        \"-pos\", pos_mode,\n",
    "        \"-e\", \"UTF-8\",\n",
    "    ]\n",
    "    if DOCTYPE.upper() in {\"NEWS\", \"COLLOQUIAL\"}:\n",
    "        cmd += [\"-dct\", DCT]\n",
    "\n",
    "    proc = subprocess.run(cmd, cwd=str(HT_DIR), capture_output=True, text=True)\n",
    "    if proc.returncode != 0:\n",
    "        raise RuntimeError(\n",
    "            \"HeidelTime failed.\\n\\nCMD:\\n\" + \" \".join(cmd) +\n",
    "            \"\\n\\nSTDOUT:\\n\" + (proc.stdout[:2000] or \"<empty>\") +\n",
    "            \"\\n\\nSTDERR:\\n\" + (proc.stderr[:2000] or \"<empty>\")\n",
    "        )\n",
    "    return proc.stdout\n",
    "\n",
    "def extract_timeml_block(text: str) -> str:\n",
    "    m = re.search(r\"<TimeML[\\s\\S]*?</TimeML>\", text)\n",
    "    if not m:\n",
    "        preview = text[:500].replace(\"\\n\", \"\\\\n\")\n",
    "        raise ValueError(f\"No <TimeML> block found in HeidelTime output. Preview: {preview}\")\n",
    "    return m.group(0)\n",
    "\n",
    "def parse_timeml_timex3(heideltime_output: str) -> pd.DataFrame:\n",
    "    timeml_xml = extract_timeml_block(heideltime_output)\n",
    "    root = ET.fromstring(timeml_xml)\n",
    "\n",
    "    rows = []\n",
    "    for timex in root.iter():\n",
    "        if timex.tag.lower().endswith(\"timex3\"):\n",
    "            rows.append({\n",
    "                \"tid\": timex.attrib.get(\"tid\"),\n",
    "                \"type\": timex.attrib.get(\"type\"),\n",
    "                \"value\": timex.attrib.get(\"value\"),\n",
    "                \"mod\": timex.attrib.get(\"mod\"),\n",
    "                \"quant\": timex.attrib.get(\"quant\"),\n",
    "                \"freq\": timex.attrib.get(\"freq\"),\n",
    "                \"beginPoint\": timex.attrib.get(\"beginPoint\"),\n",
    "                \"endPoint\": timex.attrib.get(\"endPoint\"),\n",
    "                \"text\": \"\".join(timex.itertext()).strip(),\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "example = txt_files[0]\n",
    "print(\"Example file:\", example)\n",
    "out_no = run_heideltime(example, pos_mode=\"NO\", conf_path=HT_CONF)\n",
    "df_preview = parse_timeml_timex3(out_no)\n",
    "df_preview.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507f9239",
   "metadata": {},
   "source": [
    "### FULL DATASET (ALL STATES) — SINGLE RUN (TreeTagger)\n",
    " - evaluates ALL OCR_output/<State>/*.txt exactly once\n",
    " - extracts validity fields + short/exact evidence snippet\n",
    " - records time + memory per document\n",
    " - builds per-state time/memory matrices (aggregates)\n",
    " - saves CSV + Excel (multiple sheets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f11040f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Total documents found: 298\n",
      "[INFO] Processed 50/298\n",
      "[INFO] Processed 100/298\n",
      "[INFO] Processed 150/298\n",
      "[INFO] Processed 200/298\n",
      "[INFO] Processed 250/298\n",
      "\n",
      "[OK] Saved:\n",
      " - tables\\full_dataset_treetagger_results.csv\n",
      " - tables\\full_dataset_treetagger_time_memory_per_doc.csv\n",
      " - tables\\full_dataset_treetagger_time_memory_by_state.csv\n",
      " - tables\\full_dataset_treetagger_time_memory_summary.csv\n",
      " - tables\\full_dataset_treetagger_ALL.xlsx\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_docs</th>\n",
       "      <th>n_errors</th>\n",
       "      <th>total_time_sec</th>\n",
       "      <th>avg_time_per_doc_sec</th>\n",
       "      <th>median_time_per_doc_sec</th>\n",
       "      <th>p95_time_per_doc_sec</th>\n",
       "      <th>max_py_peak_mem_mb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>298</td>\n",
       "      <td>3</td>\n",
       "      <td>895.962781</td>\n",
       "      <td>3.003188</td>\n",
       "      <td>2.752497</td>\n",
       "      <td>4.43746</td>\n",
       "      <td>0.551585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_docs  n_errors  total_time_sec  avg_time_per_doc_sec  \\\n",
       "0     298         3      895.962781              3.003188   \n",
       "\n",
       "   median_time_per_doc_sec  p95_time_per_doc_sec  max_py_peak_mem_mb  \n",
       "0                 2.752497               4.43746            0.551585  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>n_docs</th>\n",
       "      <th>n_errors</th>\n",
       "      <th>total_time_sec</th>\n",
       "      <th>avg_time_sec</th>\n",
       "      <th>median_time_sec</th>\n",
       "      <th>p95_time_sec</th>\n",
       "      <th>avg_py_peak_mem_mb</th>\n",
       "      <th>max_py_peak_mem_mb</th>\n",
       "      <th>avg_rss_delta_mb</th>\n",
       "      <th>max_rss_delta_mb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>28.044413</td>\n",
       "      <td>2.804441</td>\n",
       "      <td>2.633528</td>\n",
       "      <td>3.575370</td>\n",
       "      <td>0.062143</td>\n",
       "      <td>0.215390</td>\n",
       "      <td>0.080859</td>\n",
       "      <td>0.445312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>38.110694</td>\n",
       "      <td>3.175891</td>\n",
       "      <td>2.838027</td>\n",
       "      <td>4.758265</td>\n",
       "      <td>0.133502</td>\n",
       "      <td>0.551585</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.429688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>48.411515</td>\n",
       "      <td>2.847736</td>\n",
       "      <td>2.755438</td>\n",
       "      <td>3.558179</td>\n",
       "      <td>0.095198</td>\n",
       "      <td>0.236548</td>\n",
       "      <td>0.009651</td>\n",
       "      <td>0.027344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>27.291306</td>\n",
       "      <td>2.729131</td>\n",
       "      <td>2.588730</td>\n",
       "      <td>3.676403</td>\n",
       "      <td>0.076703</td>\n",
       "      <td>0.294326</td>\n",
       "      <td>0.007422</td>\n",
       "      <td>0.015625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California</td>\n",
       "      <td>116</td>\n",
       "      <td>0</td>\n",
       "      <td>340.659986</td>\n",
       "      <td>2.936724</td>\n",
       "      <td>2.863382</td>\n",
       "      <td>3.716802</td>\n",
       "      <td>0.102575</td>\n",
       "      <td>0.454807</td>\n",
       "      <td>0.003098</td>\n",
       "      <td>0.199219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Connecticut</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6.072587</td>\n",
       "      <td>3.036293</td>\n",
       "      <td>3.036293</td>\n",
       "      <td>3.163330</td>\n",
       "      <td>0.141356</td>\n",
       "      <td>0.180077</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.003906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Hawaii</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>9.093191</td>\n",
       "      <td>2.273298</td>\n",
       "      <td>2.268951</td>\n",
       "      <td>2.350889</td>\n",
       "      <td>0.040287</td>\n",
       "      <td>0.040615</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Idaho</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.934099</td>\n",
       "      <td>2.934099</td>\n",
       "      <td>2.934099</td>\n",
       "      <td>2.934099</td>\n",
       "      <td>0.039525</td>\n",
       "      <td>0.039525</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Illinois</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.518243</td>\n",
       "      <td>2.518243</td>\n",
       "      <td>2.518243</td>\n",
       "      <td>2.518243</td>\n",
       "      <td>0.039759</td>\n",
       "      <td>0.039759</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Indiana</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>8.360735</td>\n",
       "      <td>2.786912</td>\n",
       "      <td>2.724178</td>\n",
       "      <td>3.154888</td>\n",
       "      <td>0.047063</td>\n",
       "      <td>0.061037</td>\n",
       "      <td>0.001302</td>\n",
       "      <td>0.003906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         state  n_docs  n_errors  total_time_sec  avg_time_sec  \\\n",
       "0      Alabama      10         0       28.044413      2.804441   \n",
       "1       Alaska      12         0       38.110694      3.175891   \n",
       "2      Arizona      17         1       48.411515      2.847736   \n",
       "3     Arkansas      10         0       27.291306      2.729131   \n",
       "4   California     116         0      340.659986      2.936724   \n",
       "5  Connecticut       2         0        6.072587      3.036293   \n",
       "6       Hawaii       4         0        9.093191      2.273298   \n",
       "7        Idaho       1         0        2.934099      2.934099   \n",
       "8     Illinois       1         0        2.518243      2.518243   \n",
       "9      Indiana       3         0        8.360735      2.786912   \n",
       "\n",
       "   median_time_sec  p95_time_sec  avg_py_peak_mem_mb  max_py_peak_mem_mb  \\\n",
       "0         2.633528      3.575370            0.062143            0.215390   \n",
       "1         2.838027      4.758265            0.133502            0.551585   \n",
       "2         2.755438      3.558179            0.095198            0.236548   \n",
       "3         2.588730      3.676403            0.076703            0.294326   \n",
       "4         2.863382      3.716802            0.102575            0.454807   \n",
       "5         3.036293      3.163330            0.141356            0.180077   \n",
       "6         2.268951      2.350889            0.040287            0.040615   \n",
       "7         2.934099      2.934099            0.039525            0.039525   \n",
       "8         2.518243      2.518243            0.039759            0.039759   \n",
       "9         2.724178      3.154888            0.047063            0.061037   \n",
       "\n",
       "   avg_rss_delta_mb  max_rss_delta_mb  \n",
       "0          0.080859          0.445312  \n",
       "1          0.083333          0.429688  \n",
       "2          0.009651          0.027344  \n",
       "3          0.007422          0.015625  \n",
       "4          0.003098          0.199219  \n",
       "5          0.001953          0.003906  \n",
       "6          0.007812          0.031250  \n",
       "7          0.000000          0.000000  \n",
       "8          0.000000          0.000000  \n",
       "9          0.001302          0.003906  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>state</th>\n",
       "      <th>source_path</th>\n",
       "      <th>effective_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>duration</th>\n",
       "      <th>end_date_source</th>\n",
       "      <th>validity_status</th>\n",
       "      <th>validity_evidence</th>\n",
       "      <th>evidence_short</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama/Alabama_1</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>d:\\NLP_Project_tasks_6_8_11\\OCR_output\\Alabama...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>uncertain</td>\n",
       "      <td>[{\"text\": \"Agreement Between The State of Alab...</td>\n",
       "      <td>Agreement Between The State of Alabama, Alabam...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alabama/Alabama_10</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>d:\\NLP_Project_tasks_6_8_11\\OCR_output\\Alabama...</td>\n",
       "      <td>2007-11-16</td>\n",
       "      <td>2032-01-01</td>\n",
       "      <td>three years</td>\n",
       "      <td>None</td>\n",
       "      <td>found</td>\n",
       "      <td>[{\"text\": \"Southeastern United States - Canadi...</td>\n",
       "      <td>Southeastern United States - Canadian Province...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alabama/Alabama_2</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>d:\\NLP_Project_tasks_6_8_11\\OCR_output\\Alabama...</td>\n",
       "      <td>2007-11-01</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>found</td>\n",
       "      <td>[{\"text\": \"It is intended that the SEUS-Canadi...</td>\n",
       "      <td>It is intended that the SEUS-Canadian Province...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama/Alabama_3</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>d:\\NLP_Project_tasks_6_8_11\\OCR_output\\Alabama...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>three years</td>\n",
       "      <td>None</td>\n",
       "      <td>found</td>\n",
       "      <td>[{\"text\": \"This Memorandum of Intent shall ent...</td>\n",
       "      <td>This Memorandum of Intent shall enter into eff...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alabama/Alabama_4</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>d:\\NLP_Project_tasks_6_8_11\\OCR_output\\Alabama...</td>\n",
       "      <td>1994-01-01</td>\n",
       "      <td>1994-06-01</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>found</td>\n",
       "      <td>[{\"text\": \"MEMORANDUM OF UNDERSTANDING AND COO...</td>\n",
       "      <td>MEMORANDUM OF UNDERSTANDING AND COOPERATION BE...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               doc_id    state  \\\n",
       "0   Alabama/Alabama_1  Alabama   \n",
       "1  Alabama/Alabama_10  Alabama   \n",
       "2   Alabama/Alabama_2  Alabama   \n",
       "3   Alabama/Alabama_3  Alabama   \n",
       "4   Alabama/Alabama_4  Alabama   \n",
       "\n",
       "                                         source_path effective_date  \\\n",
       "0  d:\\NLP_Project_tasks_6_8_11\\OCR_output\\Alabama...           None   \n",
       "1  d:\\NLP_Project_tasks_6_8_11\\OCR_output\\Alabama...     2007-11-16   \n",
       "2  d:\\NLP_Project_tasks_6_8_11\\OCR_output\\Alabama...     2007-11-01   \n",
       "3  d:\\NLP_Project_tasks_6_8_11\\OCR_output\\Alabama...           None   \n",
       "4  d:\\NLP_Project_tasks_6_8_11\\OCR_output\\Alabama...     1994-01-01   \n",
       "\n",
       "     end_date     duration end_date_source validity_status  \\\n",
       "0        None         None            None       uncertain   \n",
       "1  2032-01-01  three years            None           found   \n",
       "2        None         None            None           found   \n",
       "3        None  three years            None           found   \n",
       "4  1994-06-01         None            None           found   \n",
       "\n",
       "                                   validity_evidence  \\\n",
       "0  [{\"text\": \"Agreement Between The State of Alab...   \n",
       "1  [{\"text\": \"Southeastern United States - Canadi...   \n",
       "2  [{\"text\": \"It is intended that the SEUS-Canadi...   \n",
       "3  [{\"text\": \"This Memorandum of Intent shall ent...   \n",
       "4  [{\"text\": \"MEMORANDUM OF UNDERSTANDING AND COO...   \n",
       "\n",
       "                                      evidence_short error  \n",
       "0  Agreement Between The State of Alabama, Alabam...  None  \n",
       "1  Southeastern United States - Canadian Province...  None  \n",
       "2  It is intended that the SEUS-Canadian Province...  None  \n",
       "3  This Memorandum of Intent shall enter into eff...  None  \n",
       "4  MEMORANDUM OF UNDERSTANDING AND COOPERATION BE...  None  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FULL DATASET (ALL STATES) \n",
    "# ============================================================\n",
    "\n",
    "import os, re, time, gc, json, datetime\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import tracemalloc\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "USE_CACHE = False   # <- YOU WANT THIS FALSE\n",
    "\n",
    "\n",
    "# Optional RSS memory (nice-to-have)\n",
    "try:\n",
    "    import psutil\n",
    "    _HAS_PSUTIL = True\n",
    "    _PROC = psutil.Process(os.getpid())\n",
    "except Exception:\n",
    "    _HAS_PSUTIL = False\n",
    "    _PROC = None\n",
    "\n",
    "def _rss_mb():\n",
    "    if not _HAS_PSUTIL:\n",
    "        return None\n",
    "    return _PROC.memory_info().rss / (1024 ** 2)\n",
    "\n",
    "# -----------------------\n",
    "# 0) Inputs / outputs\n",
    "# -----------------------\n",
    "OCR_ROOT = Path(OCR_DIR)  # from earlier cells\n",
    "if not OCR_ROOT.exists():\n",
    "    raise RuntimeError(f\"OCR_DIR not found: {OCR_ROOT.resolve()}\")\n",
    "\n",
    "OUT_DIR = Path(\"tables\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CACHE_DIR = Path(\"heideltime_cache\")\n",
    "CACHE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "EVIDENCE_MAX_CHARS = 120\n",
    "\n",
    "# We require TreeTagger in this notebook.\n",
    "if patched_conf is None:\n",
    "    raise RuntimeError(\"patched_conf is None — TreeTagger config not patched. Run Section 2.\")\n",
    "\n",
    "# -----------------------\n",
    "# 1) Helpers (robust TimeML parsing)\n",
    "# -----------------------\n",
    "_ILLEGAL_XML_1_0_RE = re.compile(\n",
    "    r\"[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F]\"  # illegal control chars\n",
    "    r\"|[\\uD800-\\uDFFF]\"               # surrogate blocks\n",
    "    r\"|\\uFFFE|\\uFFFF\"                 # non-characters\n",
    ")\n",
    "_AMP_NOT_ENTITY_RE = re.compile(r\"&(?!(?:amp|lt|gt|quot|apos|#\\d+|#x[0-9A-Fa-f]+);)\")\n",
    "\n",
    "def sanitize_xml_text(s: str) -> str:\n",
    "    if not s:\n",
    "        return s\n",
    "    s = _ILLEGAL_XML_1_0_RE.sub(\"\", s)\n",
    "    s = _AMP_NOT_ENTITY_RE.sub(\"&amp;\", s)\n",
    "    return s\n",
    "\n",
    "def extract_timeml_block(heideltime_output: str) -> str:\n",
    "    if not heideltime_output:\n",
    "        return \"\"\n",
    "    m = re.search(r\"(<TimeML\\b.*?</TimeML\\s*>)\", heideltime_output, flags=re.DOTALL)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "    m = re.search(r\"(<TIMEML\\b.*?</TIMEML\\s*>)\", heideltime_output, flags=re.DOTALL | re.IGNORECASE)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "    return heideltime_output\n",
    "\n",
    "def parse_timeml_timex3(heideltime_output: str) -> pd.DataFrame:\n",
    "    timeml_xml = sanitize_xml_text(extract_timeml_block(heideltime_output))\n",
    "    if \"<\" in timeml_xml:\n",
    "        timeml_xml = timeml_xml[timeml_xml.find(\"<\"):]\n",
    "    try:\n",
    "        root = ET.fromstring(timeml_xml)\n",
    "    except ET.ParseError:\n",
    "        root = ET.fromstring(sanitize_xml_text(f\"<ROOT>{timeml_xml}</ROOT>\"))\n",
    "\n",
    "    rows = []\n",
    "    for el in root.iter():\n",
    "        tag = str(el.tag)\n",
    "        if tag == \"TIMEX3\" or tag.endswith(\"TIMEX3\"):\n",
    "            rows.append({\n",
    "                \"tid\": el.attrib.get(\"tid\"),\n",
    "                \"type\": el.attrib.get(\"type\"),\n",
    "                \"value\": el.attrib.get(\"value\"),\n",
    "                \"mod\": el.attrib.get(\"mod\"),\n",
    "                \"text\": \"\".join(el.itertext()).strip(),\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# -----------------------\n",
    "# 2) Validity extraction (same logic as your old final cell)\n",
    "# -----------------------\n",
    "ANCHOR_START = re.compile(\n",
    "    r\"\\beffective\\b|\\benter\\s+into\\s+force\\b|\\benter\\s+into\\s+effect\\b|\\bas\\s+of\\b|\\bcommenc\",\n",
    "    re.I\n",
    ")\n",
    "ANCHOR_END = re.compile(\n",
    "    r\"\\buntil\\b|\\bexpire\\b|\\bexpiration\\b|\\bterminate\\b|\\btermination\\b|\\bend\\s+date\\b\",\n",
    "    re.I\n",
    ")\n",
    "\n",
    "def split_sentences(text: str):\n",
    "    text = re.sub(r\"\\s+\", \" \", (text or \"\")).strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    return re.split(r\"(?<=[\\.\\!\\?])\\s+|\\n+\", text)\n",
    "\n",
    "def normalize_timex_date(value: str):\n",
    "    if not value:\n",
    "        return None\n",
    "    v = str(value).strip()\n",
    "    if re.fullmatch(r\"\\d{4}-\\d{2}-\\d{2}\", v):\n",
    "        return v\n",
    "    if re.fullmatch(r\"\\d{4}-\\d{2}\", v):\n",
    "        return f\"{v}-01\"\n",
    "    if re.fullmatch(r\"\\d{4}\", v):\n",
    "        return f\"{v}-01-01\"\n",
    "    return None\n",
    "\n",
    "def aggregate_validity_from_heideltime(doc_text: str, timex_df: pd.DataFrame):\n",
    "    sents = split_sentences(doc_text)\n",
    "\n",
    "    candidates = []\n",
    "    for _, r in timex_df.iterrows():\n",
    "        timex_text = str(r.get(\"text\") or \"\").strip()\n",
    "        timex_type = str(r.get(\"type\") or \"\").strip().upper()\n",
    "        timex_val  = str(r.get(\"value\") or \"\").strip()\n",
    "        if not timex_text:\n",
    "            continue\n",
    "\n",
    "        sent_hit = None\n",
    "        for s in sents:\n",
    "            if timex_text and timex_text in s:\n",
    "                sent_hit = s\n",
    "                break\n",
    "\n",
    "        candidates.append({\n",
    "            \"timex_text\": timex_text,\n",
    "            \"timex_type\": timex_type,\n",
    "            \"timex_value\": timex_val,\n",
    "            \"sentence\": sent_hit,\n",
    "            \"start_anchor\": bool(sent_hit and ANCHOR_START.search(sent_hit)),\n",
    "            \"end_anchor\": bool(sent_hit and ANCHOR_END.search(sent_hit)),\n",
    "        })\n",
    "\n",
    "    duration = None\n",
    "    for c in candidates:\n",
    "        if c[\"timex_type\"] == \"DURATION\":\n",
    "            duration = c[\"timex_text\"]\n",
    "            break\n",
    "\n",
    "    date_cands = []\n",
    "    for c in candidates:\n",
    "        if c[\"timex_type\"] == \"DATE\":\n",
    "            d = normalize_timex_date(c[\"timex_value\"])\n",
    "            if d:\n",
    "                date_cands.append((d, c))\n",
    "    date_cands.sort(key=lambda x: x[0])\n",
    "\n",
    "    effective_date = None\n",
    "    end_date = None\n",
    "    end_date_source = None\n",
    "\n",
    "    for d, c in date_cands:\n",
    "        if effective_date is None and c[\"start_anchor\"]:\n",
    "            effective_date = d\n",
    "\n",
    "    for d, c in reversed(date_cands):\n",
    "        if end_date is None and c[\"end_anchor\"]:\n",
    "            end_date = d\n",
    "            end_date_source = \"explicit\"\n",
    "\n",
    "    if effective_date is None and date_cands:\n",
    "        effective_date = date_cands[0][0]\n",
    "    if end_date is None and len(date_cands) >= 2:\n",
    "        end_date = date_cands[-1][0]\n",
    "\n",
    "    has_any = bool(effective_date or end_date or duration)\n",
    "    status = \"found\" if has_any else (\"uncertain\" if candidates else \"absent\")\n",
    "\n",
    "    evidence = []\n",
    "    for c in candidates:\n",
    "        if c.get(\"sentence\"):\n",
    "            evidence.append({\n",
    "                \"text\": c[\"sentence\"],\n",
    "                \"timex_text\": c[\"timex_text\"],\n",
    "                \"timex_type\": c[\"timex_type\"],\n",
    "                \"timex_value\": c[\"timex_value\"],\n",
    "            })\n",
    "        if len(evidence) >= 5:\n",
    "            break\n",
    "\n",
    "    return {\n",
    "        \"effective_date\": effective_date,\n",
    "        \"end_date\": end_date,\n",
    "        \"duration\": duration,\n",
    "        \"end_date_source\": end_date_source,\n",
    "        \"validity_status\": status,\n",
    "        \"validity_evidence\": evidence,\n",
    "    }\n",
    "\n",
    "def evidence_short(evidence_list, max_chars=EVIDENCE_MAX_CHARS):\n",
    "    if not evidence_list:\n",
    "        return None\n",
    "    s = str(evidence_list[0].get(\"text\") or \"\").strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s[:max_chars] + (\"…\" if len(s) > max_chars else \"\")\n",
    "\n",
    "# -----------------------\n",
    "# 3) HeidelTime cache (TreeTagger mode)\n",
    "# -----------------------\n",
    "\n",
    "def get_heideltime_output(txt_path: Path):\n",
    "    cache_file = CACHE_DIR / (txt_path.name + \".TREETAGGER.timeml.txt\")\n",
    "\n",
    "    if USE_CACHE and cache_file.exists():\n",
    "        return cache_file.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "    out = run_heideltime(txt_path, pos_mode=\"TREETAGGER\", conf_path=patched_conf)\n",
    "\n",
    "    if USE_CACHE:\n",
    "        cache_file.write_text(out or \"\", encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "    return out\n",
    "\n",
    "# -----------------------\n",
    "# 4) Collect ALL documents across states\n",
    "# -----------------------\n",
    "all_txt = sorted(OCR_ROOT.glob(\"*/*.txt\"))\n",
    "if not all_txt:\n",
    "    raise RuntimeError(f\"No .txt found under: {OCR_ROOT.resolve()} (expected OCR_output/<State>/*.txt)\")\n",
    "\n",
    "print(f\"[INFO] Total documents found: {len(all_txt)}\")\n",
    "\n",
    "# -----------------------\n",
    "# 5) SINGLE RUN over whole dataset (time + memory)\n",
    "# -----------------------\n",
    "rows = []\n",
    "prof = []\n",
    "\n",
    "t0_all = time.perf_counter()\n",
    "\n",
    "for i, p in enumerate(all_txt, start=1):\n",
    "    state = p.parent.name\n",
    "    doc_id = f\"{state}/{p.stem}\"\n",
    "\n",
    "    # Read text once\n",
    "    doc_text = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "    rss_before = _rss_mb()\n",
    "    tracemalloc.start()\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    err = None\n",
    "    out = None\n",
    "    try:\n",
    "        out = get_heideltime_output(p)\n",
    "        timex_df = parse_timeml_timex3(out)\n",
    "        validity = aggregate_validity_from_heideltime(doc_text, timex_df)\n",
    "    except Exception as e:\n",
    "        validity = {\n",
    "            \"effective_date\": None,\n",
    "            \"end_date\": None,\n",
    "            \"duration\": None,\n",
    "            \"end_date_source\": None,\n",
    "            \"validity_status\": \"error\",\n",
    "            \"validity_evidence\": [{\"text\": f\"ERROR: {e}\"}],\n",
    "        }\n",
    "        err = str(e)\n",
    "\n",
    "    dt = time.perf_counter() - t0\n",
    "    cur, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    rss_after = _rss_mb()\n",
    "\n",
    "    ev_short = evidence_short(validity.get(\"validity_evidence\"))\n",
    "\n",
    "    rows.append({\n",
    "        \"doc_id\": doc_id,\n",
    "        \"state\": state,\n",
    "        \"source_path\": str(p),\n",
    "        \"effective_date\": validity.get(\"effective_date\"),\n",
    "        \"end_date\": validity.get(\"end_date\"),\n",
    "        \"duration\": validity.get(\"duration\"),\n",
    "        \"end_date_source\": validity.get(\"end_date_source\"),\n",
    "        \"validity_status\": validity.get(\"validity_status\"),\n",
    "        # store full evidence as JSON string (Excel/CSV friendly)\n",
    "        \"validity_evidence\": json.dumps(validity.get(\"validity_evidence\") or [], ensure_ascii=False),\n",
    "        \"evidence_short\": ev_short,\n",
    "        \"error\": err,\n",
    "    })\n",
    "\n",
    "    prof.append({\n",
    "        \"doc_id\": doc_id,\n",
    "        \"state\": state,\n",
    "        \"source_path\": str(p),\n",
    "        \"time_sec\": dt,\n",
    "        \"py_peak_mem_mb\": peak / (1024 ** 2),\n",
    "        \"rss_before_mb\": rss_before,\n",
    "        \"rss_after_mb\": rss_after,\n",
    "        \"rss_delta_mb\": (rss_after - rss_before) if (rss_after is not None and rss_before is not None) else None,\n",
    "        \"had_error\": bool(err),\n",
    "    })\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        gc.collect()\n",
    "        print(f\"[INFO] Processed {i}/{len(all_txt)}\")\n",
    "\n",
    "total_time = time.perf_counter() - t0_all\n",
    "\n",
    "results_df = pd.DataFrame(rows)\n",
    "prof_df = pd.DataFrame(prof)\n",
    "\n",
    "# -----------------------\n",
    "# 6) Per-state matrices (aggregates)\n",
    "# -----------------------\n",
    "state_agg = (\n",
    "    prof_df.groupby(\"state\", dropna=False)\n",
    "    .agg(\n",
    "        n_docs=(\"doc_id\", \"count\"),\n",
    "        n_errors=(\"had_error\", \"sum\"),\n",
    "        total_time_sec=(\"time_sec\", \"sum\"),\n",
    "        avg_time_sec=(\"time_sec\", \"mean\"),\n",
    "        median_time_sec=(\"time_sec\", \"median\"),\n",
    "        p95_time_sec=(\"time_sec\", lambda s: s.quantile(0.95) if s.notna().any() else None),\n",
    "        avg_py_peak_mem_mb=(\"py_peak_mem_mb\", \"mean\"),\n",
    "        max_py_peak_mem_mb=(\"py_peak_mem_mb\", \"max\"),\n",
    "        avg_rss_delta_mb=(\"rss_delta_mb\", \"mean\"),\n",
    "        max_rss_delta_mb=(\"rss_delta_mb\", \"max\"),\n",
    "    )\n",
    "    .reset_index()\n",
    "    .sort_values([\"state\"])\n",
    ")\n",
    "\n",
    "summary = pd.DataFrame([{\n",
    "    \"n_docs\": len(results_df),\n",
    "    \"n_errors\": int(prof_df[\"had_error\"].sum()),\n",
    "    \"total_time_sec\": total_time,\n",
    "    \"avg_time_per_doc_sec\": float(prof_df[\"time_sec\"].mean()),\n",
    "    \"median_time_per_doc_sec\": float(prof_df[\"time_sec\"].median()),\n",
    "    \"p95_time_per_doc_sec\": float(prof_df[\"time_sec\"].quantile(0.95)),\n",
    "    \"max_py_peak_mem_mb\": float(prof_df[\"py_peak_mem_mb\"].max()),\n",
    "}])\n",
    "\n",
    "# -----------------------\n",
    "# 7) Save CSV + Excel\n",
    "# -----------------------\n",
    "stamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "base = \"full_dataset_treetagger\"\n",
    "\n",
    "csv_results = OUT_DIR / f\"{base}_results.csv\"\n",
    "csv_prof    = OUT_DIR / f\"{base}_time_memory_per_doc.csv\"\n",
    "csv_state   = OUT_DIR / f\"{base}_time_memory_by_state.csv\"\n",
    "csv_summary = OUT_DIR / f\"{base}_time_memory_summary.csv\"\n",
    "xlsx_path   = OUT_DIR / f\"{base}_ALL.xlsx\"\n",
    "\n",
    "results_df.to_csv(csv_results, index=False, encoding=\"utf-8\")\n",
    "prof_df.to_csv(csv_prof, index=False, encoding=\"utf-8\")\n",
    "state_agg.to_csv(csv_state, index=False, encoding=\"utf-8\")\n",
    "summary.to_csv(csv_summary, index=False, encoding=\"utf-8\")\n",
    "\n",
    "with pd.ExcelWriter(xlsx_path, engine=\"openpyxl\") as w:\n",
    "    results_df.to_excel(w, index=False, sheet_name=\"results\")\n",
    "    prof_df.to_excel(w, index=False, sheet_name=\"time_memory_per_doc\")\n",
    "    state_agg.to_excel(w, index=False, sheet_name=\"time_memory_by_state\")\n",
    "    summary.to_excel(w, index=False, sheet_name=\"summary\")\n",
    "\n",
    "print(\"\\n[OK] Saved:\")\n",
    "print(\" -\", csv_results)\n",
    "print(\" -\", csv_prof)\n",
    "print(\" -\", csv_state)\n",
    "print(\" -\", csv_summary)\n",
    "print(\" -\", xlsx_path)\n",
    "\n",
    "display(summary)\n",
    "display(state_agg.head(10))\n",
    "results_df.head(5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
