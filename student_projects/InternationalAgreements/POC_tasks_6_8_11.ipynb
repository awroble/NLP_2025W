{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "985e98fe",
   "metadata": {},
   "source": [
    "# POC — International Agreements Database Mining\n",
    "\n",
    "This notebook implements a **hybrid extraction pipeline** for noisy OCR legal agreements for the following project tasks:\n",
    "- **6: Terms of validity** (start/end/duration)\n",
    "- **8: Conditions for extending** (automatic vs mutual decision vs optional)\n",
    "- **11: Evaluation of implementation** (review/audit/reporting) + basic attributes\n",
    "\n",
    "POC Pipeline:\n",
    "\n",
    "1. **Candidate retrieval** (regex triggers + neighbor expansion for recall under OCR noise)  \n",
    "2. **Neural clause detection** (finetuned legal model if available, else MNLI zero-shot baseline)  \n",
    "3. **NLI verification** (ContractNLI-style hypothesis test)  \n",
    "4. **Normalization & structured output** (dates/durations, renewal type + notice, evaluation attributes)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd25dae",
   "metadata": {},
   "source": [
    "## 1) Setup & Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e8ad07",
   "metadata": {},
   "source": [
    "### 1.1 Configuration and memory‑safe model loading\n",
    "- Set device/CPU defaults, thresholds, and model names.\n",
    "- Define shared helpers (random seed, caps, lightweight data structures)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a8641cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Config loaded | Device: CPU\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 0) CONFIG + MEMORY-SAFE MODEL LOADING \n",
    "# =========================\n",
    "import platform\n",
    "\n",
    "# Device: GPU if available, else CPU\n",
    "try:\n",
    "    import torch\n",
    "    DEVICE = 0 if torch.cuda.is_available() else -1\n",
    "except Exception:\n",
    "    DEVICE = -1\n",
    "\n",
    "# --- Switches (set these as needed) ---\n",
    "USE_FINETUNED_SEQCLS = False\n",
    "FINETUNED_SEQCLS_MODEL = None  # e.g. \"nlpaueb/legal-bert-base-uncased\" or your finetuned checkpoint\n",
    "\n",
    "USE_ZEROSHOT_FALLBACK = True\n",
    "# Default to SMALL on Windows/CPU to avoid paging-file OSError 1455\n",
    "ZEROSHOT_MODEL = \"typeform/distilbert-base-uncased-mnli\"\n",
    "\n",
    "USE_NLI_VERIFIER = True\n",
    "NLI_MODEL = \"typeform/distilbert-base-uncased-mnli\"\n",
    "\n",
    "# Thresholds (tune on a dev set)\n",
    "THRESH_TEMPORAL = 0.65\n",
    "THRESH_RENEWAL = 0.60\n",
    "THRESH_EVAL = 0.60\n",
    "\n",
    "# Retrieval\n",
    "NEIGHBOR_K = 2\n",
    "MAX_CANDIDATES = 50\n",
    "\n",
    "# Optional HeidelTime hook (off by default)\n",
    "USE_HEIDELTIME = False\n",
    "HEIDELTIME_JAR = None\n",
    "HEIDELTIME_CONFIG = None\n",
    "\n",
    "# Keep backward-compatible names used later in the notebook\n",
    "CLAUSE_MODEL = ZEROSHOT_MODEL\n",
    "\n",
    "# -------------------------\n",
    "# Memory-safe pipeline loader\n",
    "# -------------------------\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "def make_zsc(model_name: str, device: int):\n",
    "    \"\"\"\n",
    "    Create zero-shot-classification pipeline with safe fallbacks.\n",
    "    Avoids Windows paging-file OSError by falling back to smaller MNLI models.\n",
    "    \"\"\"\n",
    "    fallbacks = [\n",
    "        model_name,\n",
    "        \"typeform/distilbert-base-uncased-mnli\",\n",
    "        \"valhalla/distilbart-mnli-12-1\",\n",
    "    ]\n",
    "    last_err = None\n",
    "    for name in fallbacks:\n",
    "        try:\n",
    "            print(f\"Loading ZSC model: {name}\")\n",
    "            return pipeline(\n",
    "                \"zero-shot-classification\",\n",
    "                model=name,\n",
    "                device=device,\n",
    "                model_kwargs={\"low_cpu_mem_usage\": True},\n",
    "            )\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            print(f\"⚠️ Failed loading {name}: {e}\")\n",
    "    raise RuntimeError(f\"Failed to load any ZSC model. Last error: {last_err}\")\n",
    "\n",
    "print(\"✅ Config loaded | Device:\", \"GPU\" if DEVICE==0 else \"CPU\")\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Dict, Optional, Tuple, Any\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import dateparser\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16149dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# CONFIG (CPU)\n",
    "# -----------------------------\n",
    "DEVICE = -1\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Retrieval caps\n",
    "MAX_BLOCKS = 24\n",
    "MAX_SENTS  = 80\n",
    "NEIGHBOR_K = 2\n",
    "\n",
    "# Thresholds\n",
    "THRESH_TEMPORAL = 0.65   # for validity (temporal clause)\n",
    "THRESH_RENEWAL  = 0.60   # renewal type\n",
    "THRESH_EVAL     = 0.60   # eval present\n",
    "\n",
    "DERIVE_END_DATE = True\n",
    "\n",
    "# ------------- MODEL CHOICES -------------\n",
    "# 1) Clause classifier (SOTA: LegalBERT fine-tuned token/sequence classification)\n",
    "#    For CPU POC, use a smaller sequence classifier OR keep ZSC as fallback.\n",
    "#\n",
    "# Recommended: swap this to a LegalBERT-like finetune when you have one.\n",
    "CLAUSE_MODEL = \"typeform/distilbert-base-uncased-mnli\"  # fallback verifier/classifier\n",
    "\n",
    "# Optional NLI verifier (ContractNLI-like verification layer)\n",
    "USE_NLI_VERIFIER = True\n",
    "NLI_MODEL = \"typeform/distilbert-base-uncased-mnli\"  # swap to stronger MNLI later (DeBERTa MNLI)\n",
    "\n",
    "# If you later have a real finetuned model for \"temporal/renewal/eval\", plug it here:\n",
    "# SEQCLS_MODEL = \"your-finetuned-legalbert-clause-classifier\"\n",
    "USE_SEQCLS = False\n",
    "SEQCLS_MODEL = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a151fa9e",
   "metadata": {},
   "source": [
    "### 1.2 Data I/O\n",
    "- Load OCR text (plain text or JSON) into a single normalized string.\n",
    "- Preserve any page markers used later for evidence and page‑level outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a57f2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_txt(path: str) -> str:\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def load_ocr_json_with_pages(path: str) -> str:\n",
    "    \"\"\"\n",
    "    Reconstruct text with explicit page markers to preserve provenance:\n",
    "    [[PAGE=1]] ... [[PAGE=2]] ...\n",
    "    \"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    out_lines = []\n",
    "    pages = data.get(\"pages\", [])\n",
    "    for p_idx, page in enumerate(pages, start=1):\n",
    "        out_lines.append(f\"[[PAGE={p_idx}]]\")\n",
    "        for block in page.get(\"blocks\", []):\n",
    "            for line in block.get(\"lines\", []):\n",
    "                words = [w.get(\"value\", \"\") for w in line.get(\"words\", []) if w.get(\"value\")]\n",
    "                if words:\n",
    "                    out_lines.append(\" \".join(words))\n",
    "        out_lines.append(\"\")  # blank line between pages\n",
    "\n",
    "    return \"\\n\".join(out_lines)\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    text = text.replace(\"\\x0c\", \"\\n\")\n",
    "    # Keep page markers intact\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b739aae7",
   "metadata": {},
   "source": [
    "### 1.3 Page‑aware segmentation\n",
    "- Split OCR text into pages, then into blocks and sentences.\n",
    "- Keep `(page, block_id, sent_id)` so every extracted field can cite evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c264baa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAGE_MARKER = re.compile(r\"\\[\\[PAGE=(\\d+)\\]\\]\")\n",
    "\n",
    "def split_into_pages(text: str) -> List[Tuple[int, str]]:\n",
    "    \"\"\"\n",
    "    Returns [(page_num, page_text), ...]\n",
    "    If no markers exist, treat as page 1.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    matches = list(PAGE_MARKER.finditer(text))\n",
    "    if not matches:\n",
    "        return [(1, text)]\n",
    "\n",
    "    for i, m in enumerate(matches):\n",
    "        page_num = int(m.group(1))\n",
    "        start = m.end()\n",
    "        end = matches[i+1].start() if i+1 < len(matches) else len(text)\n",
    "        page_text = text[start:end].strip()\n",
    "        chunks.append((page_num, page_text))\n",
    "    return chunks\n",
    "\n",
    "# OCR-friendly sentence split: don't rely on capitalization\n",
    "_SENT_SPLIT = re.compile(r\"(?<=[\\.\\?\\!])\\s+|\\n+\")\n",
    "\n",
    "@dataclass\n",
    "class SentItem:\n",
    "    sid: int\n",
    "    page: int\n",
    "    text: str\n",
    "\n",
    "@dataclass\n",
    "class BlockItem:\n",
    "    bid: int\n",
    "    page: int\n",
    "    text: str\n",
    "\n",
    "def split_sentences_with_meta(text: str, max_len: int = 1200) -> List[SentItem]:\n",
    "    pages = split_into_pages(text)\n",
    "    sents: List[SentItem] = []\n",
    "    sid = 0\n",
    "    for page_num, page_text in pages:\n",
    "        raw = [s.strip() for s in _SENT_SPLIT.split(page_text) if s and s.strip()]\n",
    "        # Light merge for very short OCR fragments\n",
    "        merged, buf = [], \"\"\n",
    "        for s in raw:\n",
    "            if not buf:\n",
    "                buf = s\n",
    "            elif len(buf) < 120 and len(s) < 260:\n",
    "                buf = buf + \" \" + s\n",
    "            else:\n",
    "                merged.append(buf.strip())\n",
    "                buf = s\n",
    "        if buf.strip():\n",
    "            merged.append(buf.strip())\n",
    "\n",
    "        for m in merged:\n",
    "            sents.append(SentItem(sid=sid, page=page_num, text=m[:max_len]))\n",
    "            sid += 1\n",
    "    return sents\n",
    "\n",
    "def split_blocks_with_meta(text: str, max_len: int = 2500) -> List[BlockItem]:\n",
    "    pages = split_into_pages(text)\n",
    "    blocks: List[BlockItem] = []\n",
    "    bid = 0\n",
    "    for page_num, page_text in pages:\n",
    "        paras = [b.strip() for b in re.split(r\"\\n\\s*\\n+\", page_text) if b and b.strip()]\n",
    "        for b in paras:\n",
    "            b = re.sub(r\"\\s+\", \" \", b).strip()\n",
    "            if len(b) <= max_len:\n",
    "                blocks.append(BlockItem(bid=bid, page=page_num, text=b))\n",
    "                bid += 1\n",
    "            else:\n",
    "                for i in range(0, len(b), 2000):\n",
    "                    blocks.append(BlockItem(bid=bid, page=page_num, text=b[i:i+2000]))\n",
    "                    bid += 1\n",
    "    return blocks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e267e2",
   "metadata": {},
   "source": [
    "## 2) Candidate retrieval (high‑recall)\n",
    "- Use regex patterns to over‑generate candidate clauses for **validity**, **renewal**, and **evaluation**.\n",
    "- Retrieve the top candidate sentences/blocks + neighboring context for downstream scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a92a866",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDITY_PATTERNS = [\n",
    "    r\"\\bterm\\s+of\\s+validity\\b\",\n",
    "    r\"\\bterm\\s+of\\s+(this\\s+)?agreement\\b\",\n",
    "    r\"\\beffective\\s+date\\b\",\n",
    "    r\"\\beffective\\s+upon\\b\",\n",
    "    r\"\\benter\\s+into\\s+force\\b\",\n",
    "    r\"\\benter\\s+into\\s+effect\\b\",\n",
    "    r\"\\bshall\\s+remain\\s+in\\s+(force|effect)\\b\",\n",
    "    r\"\\bremain\\s+in\\s+(force|effect)\\b\",\n",
    "    r\"\\bfor\\s+a\\s+period\\s+of\\b\",\n",
    "    r\"\\bperiod\\s+of\\b\",\n",
    "    r\"\\buntil\\b\",\n",
    "    r\"\\bexpires?\\b\",\n",
    "    r\"\\bexpiration\\b\",\n",
    "    r\"\\btermination\\b\",\n",
    "    r\"\\bupon\\s+signature\\b\",\n",
    "    r\"\\bdate\\s+of\\s+signature\\b\",\n",
    "]\n",
    "\n",
    "RENEWAL_PATTERNS = [\n",
    "    r\"\\brenew(al|ed|s|ing)?\\b\",\n",
    "    r\"\\bextend(ed|s|ing)?\\b\",\n",
    "    r\"\\bextension\\b\",\n",
    "    r\"\\bautomatic(ally)?\\s+renew\\b\",\n",
    "    r\"\\bshall\\s+be\\s+renewed\\b\",\n",
    "    r\"\\bmay\\s+be\\s+renewed\\b\",\n",
    "    r\"\\bnon-?renewal\\b\",\n",
    "    r\"\\bunless\\s+terminated\\b\",\n",
    "    r\"\\bnotice\\b\",\n",
    "    r\"\\botherwise\\s+agreed\\s+upon\\b\",\n",
    "    r\"\\bby\\s+mutual\\s+agreement\\b\",\n",
    "    r\"\\bmutually\\s+agreed\\b\",\n",
    "]\n",
    "\n",
    "EVAL_PATTERNS = [\n",
    "    r\"\\bevaluat(e|ion|ing)\\b\",\n",
    "    r\"\\breview(s|ed|ing)?\\b\",\n",
    "    r\"\\bassess(ment|es|ed|ing)?\\b\",\n",
    "    r\"\\bmonitor(ing|ed|s)?\\b\",\n",
    "    r\"\\baudit(s|ed|ing)?\\b\",\n",
    "    r\"\\breport(s|ed|ing)?\\b\",\n",
    "    r\"\\bprogress\\s+report\\b\",\n",
    "    r\"\\bimplementation\\b.*\\b(review|evaluation|assessment|monitor|audit|report)\\b\",\n",
    "]\n",
    "\n",
    "def any_match(text: str, patterns: List[str]) -> bool:\n",
    "    lt = text.lower()\n",
    "    return any(re.search(p, lt) for p in patterns)\n",
    "\n",
    "# HeidelTime-inspired temporal extraction (rules baseline)\n",
    "MONTHY_DATE = re.compile(\n",
    "    r\"\\b(?:jan(?:uary)?|feb(?:ruary)?|mar(?:ch)?|apr(?:il)?|may|jun(?:e)?|\"\n",
    "    r\"jul(?:y)?|aug(?:ust)?|sep(?:tember)?|oct(?:ober)?|nov(?:ember)?|dec(?:ember)?)\"\n",
    "    r\"\\s+\\d{1,2},?\\s+\\d{4}\\b\", re.IGNORECASE\n",
    ")\n",
    "ORDINAL_MONTHY_DATE = re.compile(\n",
    "    r\"\\b(?:jan(?:uary)?|feb(?:ruary)?|mar(?:ch)?|apr(?:il)?|may|jun(?:e)?|\"\n",
    "    r\"jul(?:y)?|aug(?:ust)?|sep(?:tember)?|oct(?:ober)?|nov(?:ember)?|dec(?:ember)?)\"\n",
    "    r\"\\s+\\d{1,2}(?:st|nd|rd|th)?\\s*,?\\s*\\d{4}\\b\", re.IGNORECASE\n",
    ")\n",
    "DAY_OF_MONTH_DATE = re.compile(\n",
    "    r\"\\b\\d{1,2}(?:st|nd|rd|th)?\\s+of\\s+\"\n",
    "    r\"(?:jan(?:uary)?|feb(?:ruary)?|mar(?:ch)?|apr(?:il)?|may|jun(?:e)?|\"\n",
    "    r\"jul(?:y)?|aug(?:ust)?|sep(?:tember)?|oct(?:ober)?|nov(?:ember)?|dec(?:ember)?)\"\n",
    "    r\"(?:\\s+in\\s+the\\s+year\\s+of)?\\s+\\d{4}\\b\", re.IGNORECASE\n",
    ")\n",
    "NUM_DATE = re.compile(r\"\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b\")\n",
    "\n",
    "DURATION_NUMERIC = re.compile(r\"\\b(\\d+)\\s*(?:\\(\\s*\\d+\\s*\\)\\s*)?(years?|months?|days?)\\b\", re.IGNORECASE)\n",
    "NOTICE_PERIOD = re.compile(r\"\\b(\\d+)\\s*(?:\\(\\s*\\d+\\s*\\)\\s*)?(days?|months?|years?)\\s+(?:prior|before)\\b\", re.IGNORECASE)\n",
    "\n",
    "NUMBER_WORDS = {\n",
    "    \"one\": 1, \"two\": 2, \"three\": 3, \"four\": 4, \"five\": 5,\n",
    "    \"six\": 6, \"seven\": 7, \"eight\": 8, \"nine\": 9, \"ten\": 10,\n",
    "    \"eleven\": 11, \"twelve\": 12\n",
    "}\n",
    "WORD_DURATION = re.compile(r\"\\b(\" + \"|\".join(NUMBER_WORDS.keys()) + r\")\\s+(years?|months?|days?)\\b\", re.IGNORECASE)\n",
    "WORD_PAREN_DURATION = re.compile(r\"\\b(\" + \"|\".join(NUMBER_WORDS.keys()) + r\")\\s*\\(\\s*(\\d+)\\s*\\)\\s*(years?|months?|days?)\\b\", re.IGNORECASE)\n",
    "\n",
    "# Anchors for safe derivation\n",
    "ANCHOR_START = re.compile(r\"\\beffective\\s+date\\b|\\beffective\\s+upon\\b|\\benter\\s+into\\s+(force|effect)\\b|\\bupon\\s+signature\\b\", re.IGNORECASE)\n",
    "ANCHOR_TERM  = re.compile(r\"\\bterm\\b|\\bfor\\s+a\\s+period\\s+of\\b|\\bshall\\s+remain\\s+in\\s+(force|effect)\\b|\\buntil\\b|\\bexpires?\\b|\\bexpiration\\b\", re.IGNORECASE)\n",
    "\n",
    "def parse_dates_from_text(text: str) -> List[str]:\n",
    "    found = []\n",
    "    for rgx in [MONTHY_DATE, ORDINAL_MONTHY_DATE, DAY_OF_MONTH_DATE, NUM_DATE]:\n",
    "        for m in rgx.findall(text):\n",
    "            # Try MDY then DMY (OCR/intl ambiguity)\n",
    "            dt = dateparser.parse(m, settings={\"DATE_ORDER\": \"MDY\"})\n",
    "            if not dt:\n",
    "                dt = dateparser.parse(m, settings={\"DATE_ORDER\": \"DMY\"})\n",
    "            if dt:\n",
    "                found.append(dt.date().isoformat())\n",
    "    # de-dup stable order\n",
    "    return sorted(set(found))\n",
    "\n",
    "def parse_duration(text: str) -> Optional[str]:\n",
    "    m = DURATION_NUMERIC.search(text)\n",
    "    if m:\n",
    "        return f\"{m.group(1)} {m.group(2).lower()}\"\n",
    "    m2 = WORD_PAREN_DURATION.search(text)\n",
    "    if m2:\n",
    "        return f\"{int(m2.group(2))} {m2.group(3).lower()}\"\n",
    "    m3 = WORD_DURATION.search(text)\n",
    "    if m3:\n",
    "        return f\"{NUMBER_WORDS[m3.group(1).lower()]} {m3.group(2).lower()}\"\n",
    "    return None\n",
    "\n",
    "def parse_notice_period(text: str) -> Optional[str]:\n",
    "    m = NOTICE_PERIOD.search(text)\n",
    "    if m:\n",
    "        return f\"{m.group(1)} {m.group(2).lower()}\"\n",
    "    return None\n",
    "\n",
    "def derive_end_date_safe(effective_date: Optional[str], duration: Optional[str], evidence_text: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Only derive end_date if:\n",
    "    - we have effective_date + duration\n",
    "    - and the evidence has both start anchor AND term anchor (avoid mixing notice/training periods)\n",
    "    \"\"\"\n",
    "    if not effective_date or not duration:\n",
    "        return None\n",
    "    if not (ANCHOR_START.search(evidence_text) and ANCHOR_TERM.search(evidence_text)):\n",
    "        return None\n",
    "\n",
    "    m = re.match(r\"^\\s*(\\d+)\\s+(years?|months?|days?)\\s*$\", duration.strip(), re.IGNORECASE)\n",
    "    if not m:\n",
    "        return None\n",
    "    n = int(m.group(1))\n",
    "    unit = m.group(2).lower()\n",
    "\n",
    "    try:\n",
    "        start = datetime.fromisoformat(effective_date).date()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    if unit.startswith(\"year\"):\n",
    "        end = start + relativedelta(years=n)\n",
    "    elif unit.startswith(\"month\"):\n",
    "        end = start + relativedelta(months=n)\n",
    "    else:\n",
    "        end = start + relativedelta(days=n)\n",
    "\n",
    "    return end.isoformat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5b642b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_candidates_with_meta(\n",
    "    text: str,\n",
    "    patterns: List[str],\n",
    "    max_blocks: int = MAX_BLOCKS,\n",
    "    max_sents: int = MAX_SENTS,\n",
    "    neighbor_k: int = NEIGHBOR_K\n",
    ") -> Dict[str, Any]:\n",
    "    blocks = split_blocks_with_meta(text)\n",
    "    sents  = split_sentences_with_meta(text)\n",
    "\n",
    "    block_hits: List[BlockItem] = []\n",
    "    for b in blocks:\n",
    "        if any_match(b.text, patterns):\n",
    "            block_hits.append(b)\n",
    "            if len(block_hits) >= max_blocks:\n",
    "                break\n",
    "\n",
    "    hit_sids = []\n",
    "    for s in sents:\n",
    "        if any_match(s.text, patterns):\n",
    "            hit_sids.append(s.sid)\n",
    "            if len(hit_sids) >= max_sents:\n",
    "                break\n",
    "\n",
    "    expanded = set()\n",
    "    for sid in hit_sids:\n",
    "        for j in range(max(0, sid-neighbor_k), min(len(sents), sid+neighbor_k+1)):\n",
    "            expanded.add(j)\n",
    "\n",
    "    sent_hits = [sents[i] for i in sorted(expanded)]\n",
    "    return {\"blocks\": block_hits, \"sentences\": sent_hits, \"all_sentences\": sents}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b3b30b",
   "metadata": {},
   "source": [
    "## 2.2 Semantic filtering (SOTA proxy)\n",
    "- Load a zero‑shot **NLI/MNLI** model as a proxy for fine‑tuned **LegalBERT/Longformer** clause classifiers.\n",
    "- Optionally run a second NLI verification pass (ContractNLI‑style hypothesis check)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09e780c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ZSC model: typeform/distilbert-base-uncased-mnli\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ZSC model: typeform/distilbert-base-uncased-mnli\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Fallback: use MNLI zero-shot as \"clause classifier\" (POC)\n",
    "zsc = make_zsc(CLAUSE_MODEL, DEVICE)\n",
    "\n",
    "if USE_NLI_VERIFIER:\n",
    "    nli = make_zsc(NLI_MODEL, DEVICE)\n",
    "else:\n",
    "    nli = None\n",
    "\n",
    "def zsc_best(texts: List[str], labels: List[str]) -> List[Tuple[str, float]]:\n",
    "    if not texts:\n",
    "        return []\n",
    "    res = zsc(texts, candidate_labels=labels, multi_label=False)\n",
    "    return [(r[\"labels\"][0], float(r[\"scores\"][0])) for r in res]\n",
    "\n",
    "def nli_verify(texts: List[str], hypothesis_pos: str, hypothesis_neg: str) -> List[Tuple[bool, float]]:\n",
    "    \"\"\"\n",
    "    ContractNLI-like verification: \"entailed or not\".\n",
    "    Using ZSC MNLI as proxy:\n",
    "    - candidate_labels = [pos, neg]\n",
    "    - choose pos => verified True\n",
    "    \"\"\"\n",
    "    if not USE_NLI_VERIFIER or nli is None:\n",
    "        return [(True, 1.0) for _ in texts]  # no-op verifier for ablation\n",
    "\n",
    "    res = nli(texts, candidate_labels=[hypothesis_pos, hypothesis_neg], multi_label=False)\n",
    "    out = []\n",
    "    for r in res:\n",
    "        lab = r[\"labels\"][0]\n",
    "        sc  = float(r[\"scores\"][0])\n",
    "        out.append((lab == hypothesis_pos, sc))\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a687380c",
   "metadata": {},
   "source": [
    "### 2.3 Clause labels and scoring\n",
    "- Define label sets for each task (validity / renewal / evaluation).\n",
    "- Pick the best label per candidate and keep calibrated confidence scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49392cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1: clause type classification (cheap)\n",
    "VALIDITY_LABELS = [\n",
    "    \"Validity/term clause (effective date, duration, expiration, remain in force)\",\n",
    "    \"Other\"\n",
    "]\n",
    "RENEWAL_LABELS = [\n",
    "    \"Automatic renewal unless terminated or notice is given\",\n",
    "    \"Renewal/extension requires mutual agreement\",\n",
    "    \"Unilateral renewal/extension option\",\n",
    "    \"Other\"\n",
    "]\n",
    "EVAL_LABELS = [\n",
    "    \"Evaluation/monitoring/reporting/audit/review obligation\",\n",
    "    \"Other\"\n",
    "]\n",
    "\n",
    "# Stage 2: verification (ContractNLI-like)\n",
    "H_VALID_POS = \"This text states the start date, end date, expiration, or duration of the agreement.\"\n",
    "H_VALID_NEG = \"This text is not about the agreement's validity period.\"\n",
    "\n",
    "H_EVAL_POS  = \"This text requires evaluation, monitoring, reporting, auditing, or review of implementation.\"\n",
    "H_EVAL_NEG  = \"This text is not about evaluation, monitoring, or reporting obligations.\"\n",
    "\n",
    "H_REN_POS   = \"This text describes how the agreement is renewed or extended (automatic, mutual, or unilateral).\"\n",
    "H_REN_NEG   = \"This text is not about renewal or extension.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff59559",
   "metadata": {},
   "source": [
    "### 2.4 Evidence schema\n",
    "- Standardize evidence items: text span, page number, source (sentence/block), label, score.\n",
    "- Store whether the span was NLI‑verified and the verification score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e03630f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EvidenceItem:\n",
    "    text: str\n",
    "    page: int\n",
    "    sid: Optional[int]     # sentence id if sentence evidence\n",
    "    bid: Optional[int]     # block id if block evidence\n",
    "    source: str            # \"sentence\" | \"block\"\n",
    "    label: str\n",
    "    score: float\n",
    "    verified: bool\n",
    "    verify_score: float\n",
    "\n",
    "def build_evidence(\n",
    "    sent_items: List[SentItem],\n",
    "    block_items: List[BlockItem],\n",
    "    labels: List[str],\n",
    "    hyp_pos: str,\n",
    "    hyp_neg: str,\n",
    "    threshold: float\n",
    ") -> List[EvidenceItem]:\n",
    "    # Stage 1 classification (type)\n",
    "    sent_texts  = [s.text for s in sent_items]\n",
    "    block_texts = [b.text for b in block_items]\n",
    "\n",
    "    sent_preds  = zsc_best(sent_texts, labels)\n",
    "    block_preds = zsc_best(block_texts, labels)\n",
    "\n",
    "    # Stage 2 verification\n",
    "    sent_ver = nli_verify(sent_texts, hyp_pos, hyp_neg)\n",
    "    block_ver = nli_verify(block_texts, hyp_pos, hyp_neg)\n",
    "\n",
    "    out: List[EvidenceItem] = []\n",
    "    for s, (lab, sc), (ok, vsc) in zip(sent_items, sent_preds, sent_ver):\n",
    "        out.append(EvidenceItem(\n",
    "            text=s.text, page=s.page, sid=s.sid, bid=None, source=\"sentence\",\n",
    "            label=lab, score=sc, verified=ok and vsc >= threshold, verify_score=vsc\n",
    "        ))\n",
    "    for b, (lab, sc), (ok, vsc) in zip(block_items, block_preds, block_ver):\n",
    "        out.append(EvidenceItem(\n",
    "            text=b.text, page=b.page, sid=None, bid=b.bid, source=\"block\",\n",
    "            label=lab, score=sc, verified=ok and vsc >= threshold, verify_score=vsc\n",
    "        ))\n",
    "\n",
    "    out.sort(key=lambda x: (x.verified, x.verify_score, x.score), reverse=True)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bed5ce",
   "metadata": {},
   "source": [
    "## 3) Extraction — Validity / Renewal / Evaluation\n",
    "- Run task-specific extractors that combine: candidate retrieval → semantic filtering → normalization.\n",
    "- Each extractor returns structured fields **plus evidence** (page + snippet) for auditability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffe22286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_validity(text: str) -> Dict[str, Any]:\n",
    "    seg = retrieve_candidates_with_meta(text, VALIDITY_PATTERNS)\n",
    "    sents = seg[\"sentences\"]\n",
    "    blocks = seg[\"blocks\"]\n",
    "\n",
    "    evidence = build_evidence(\n",
    "        sent_items=sents,\n",
    "        block_items=blocks,\n",
    "        labels=VALIDITY_LABELS,\n",
    "        hyp_pos=H_VALID_POS,\n",
    "        hyp_neg=H_VALID_NEG,\n",
    "        threshold=THRESH_TEMPORAL\n",
    "    )\n",
    "\n",
    "    verified = [e for e in evidence if e.verified]\n",
    "    top = (verified[:8] if verified else evidence[:6])\n",
    "\n",
    "    effective_date = None\n",
    "    end_date = None\n",
    "    duration = None\n",
    "    end_date_source = None\n",
    "\n",
    "    # Extract from best verified evidence first\n",
    "    for e in top:\n",
    "        # pull duration early\n",
    "        if duration is None:\n",
    "            duration = parse_duration(e.text)\n",
    "        dates = parse_dates_from_text(e.text)\n",
    "\n",
    "        # Effective date candidates\n",
    "        if effective_date is None and ANCHOR_START.search(e.text) and dates:\n",
    "            effective_date = dates[0]\n",
    "\n",
    "        # End date candidates\n",
    "        if end_date is None and ANCHOR_TERM.search(e.text) and re.search(r\"\\buntil\\b|\\bexpires?\\b|\\bexpiration\\b\", e.text, re.IGNORECASE) and dates:\n",
    "            end_date = dates[-1]\n",
    "            end_date_source = \"explicit\"\n",
    "\n",
    "    # Fallback: if we have multiple dates in verified evidence, pick earliest as start and latest as end (only if verified exists)\n",
    "    if verified:\n",
    "        all_dates = []\n",
    "        for e in verified[:10]:\n",
    "            all_dates += parse_dates_from_text(e.text)\n",
    "        all_dates = sorted(set(all_dates))\n",
    "        if effective_date is None and all_dates:\n",
    "            effective_date = all_dates[0]\n",
    "        if end_date is None and len(all_dates) >= 2:\n",
    "            end_date = all_dates[-1]\n",
    "            end_date_source = \"explicit\"\n",
    "\n",
    "    # Safe derivation (only if evidence supports it)\n",
    "    if DERIVE_END_DATE and end_date is None and effective_date and duration and verified:\n",
    "        derived = derive_end_date_safe(effective_date, duration, verified[0].text)\n",
    "        if derived:\n",
    "            end_date = derived\n",
    "            end_date_source = \"derived\"\n",
    "\n",
    "    status = \"found\" if (effective_date or end_date or duration) else (\"uncertain\" if (sents or blocks) else \"absent\")\n",
    "\n",
    "    return {\n",
    "        \"effective_date\": effective_date,\n",
    "        \"end_date\": end_date,\n",
    "        \"duration\": duration,\n",
    "        \"end_date_source\": end_date_source,\n",
    "        \"validity_status\": status,\n",
    "        \"validity_evidence\": [asdict(e) for e in top]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c88f04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_renewal(text: str) -> Dict[str, Any]:\n",
    "    seg = retrieve_candidates_with_meta(text, RENEWAL_PATTERNS)\n",
    "    sents = seg[\"sentences\"]\n",
    "    blocks = seg[\"blocks\"]\n",
    "\n",
    "    if not (sents or blocks):\n",
    "        return {\n",
    "            \"renewal_type\": \"absent\",\n",
    "            \"notice_period\": None,\n",
    "            \"renewal_status\": \"absent\",\n",
    "            \"renewal_evidence\": []\n",
    "        }\n",
    "\n",
    "    evidence = build_evidence(\n",
    "        sent_items=sents,\n",
    "        block_items=blocks,\n",
    "        labels=RENEWAL_LABELS,\n",
    "        hyp_pos=H_REN_POS,\n",
    "        hyp_neg=H_REN_NEG,\n",
    "        threshold=THRESH_RENEWAL\n",
    "    )\n",
    "    verified = [e for e in evidence if e.verified]\n",
    "    top = (verified[:10] if verified else evidence[:6])\n",
    "\n",
    "    # Document-level decision: any verified evidence triggers type\n",
    "    # Priority: automatic > mutual > unilateral (you can justify this in thesis)\n",
    "    renewal_type = \"uncertain\"\n",
    "    if any(e.label == RENEWAL_LABELS[0] for e in verified):\n",
    "        renewal_type = \"automatic\"\n",
    "    elif any(e.label == RENEWAL_LABELS[1] for e in verified):\n",
    "        renewal_type = \"by_mutual_agreement\"\n",
    "    elif any(e.label == RENEWAL_LABELS[2] for e in verified):\n",
    "        renewal_type = \"unilateral_option\"\n",
    "\n",
    "    # Notice period extraction: scan verified evidence, then top evidence\n",
    "    notice = None\n",
    "    for e in (verified[:12] + top[:8]):\n",
    "        notice = notice or parse_notice_period(e.text)\n",
    "        if notice:\n",
    "            break\n",
    "\n",
    "    # If patterns hit but no verified => uncertain\n",
    "    status = \"found\" if (renewal_type != \"uncertain\") else \"uncertain\"\n",
    "\n",
    "    return {\n",
    "        \"renewal_type\": renewal_type,\n",
    "        \"notice_period\": notice,\n",
    "        \"renewal_status\": status,\n",
    "        \"renewal_evidence\": [asdict(e) for e in top]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9311e49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_evaluation(text: str) -> Dict[str, Any]:\n",
    "    seg = retrieve_candidates_with_meta(text, EVAL_PATTERNS)\n",
    "    sents = seg[\"sentences\"]\n",
    "    blocks = seg[\"blocks\"]\n",
    "\n",
    "    if not (sents or blocks):\n",
    "        return {\n",
    "            \"evaluation\": \"absent\",\n",
    "            \"evaluation_status\": \"absent\",\n",
    "            \"evaluation_evidence\": []\n",
    "        }\n",
    "\n",
    "    evidence = build_evidence(\n",
    "        sent_items=sents,\n",
    "        block_items=blocks,\n",
    "        labels=EVAL_LABELS,\n",
    "        hyp_pos=H_EVAL_POS,\n",
    "        hyp_neg=H_EVAL_NEG,\n",
    "        threshold=THRESH_EVAL\n",
    "    )\n",
    "    verified = [e for e in evidence if e.verified]\n",
    "    top = (verified[:8] if verified else evidence[:6])\n",
    "\n",
    "    if verified:\n",
    "        return {\n",
    "            \"evaluation\": \"present\",\n",
    "            \"evaluation_status\": \"found\",\n",
    "            \"evaluation_evidence\": [asdict(e) for e in top]\n",
    "        }\n",
    "    else:\n",
    "        # candidates existed but verifier didn't confirm -> uncertain (not absent)\n",
    "        return {\n",
    "            \"evaluation\": \"uncertain\",\n",
    "            \"evaluation_status\": \"uncertain\",\n",
    "            \"evaluation_evidence\": [asdict(e) for e in top]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9e2a20",
   "metadata": {},
   "source": [
    "## 4) Baselines (for ablations)\n",
    "- **Keyword baseline:** simple presence/absence using the same pattern lists.\n",
    "- **Rules-only temporal baseline:** HeidelTime-inspired date/duration parsing without ML, used as a comparison point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9983d79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_keyword(text: str, patterns: List[str]) -> bool:\n",
    "    return any_match(text, patterns)\n",
    "\n",
    "def baseline_temporal_rules(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    HeidelTime-inspired: extract dates/durations from high-recall candidate regions.\n",
    "    No ML. Useful as rule baseline in thesis.\n",
    "    \"\"\"\n",
    "    seg = retrieve_candidates_with_meta(text, VALIDITY_PATTERNS)\n",
    "    cands = [s.text for s in seg[\"sentences\"][:30]] + [b.text for b in seg[\"blocks\"][:12]]\n",
    "\n",
    "    all_dates = []\n",
    "    for c in cands:\n",
    "        all_dates += parse_dates_from_text(c)\n",
    "    all_dates = sorted(set(all_dates))\n",
    "\n",
    "    duration = None\n",
    "    for c in cands:\n",
    "        duration = duration or parse_duration(c)\n",
    "\n",
    "    effective_date = all_dates[0] if all_dates else None\n",
    "    end_date = all_dates[-1] if len(all_dates) >= 2 else None\n",
    "    end_date_source = \"explicit\" if end_date else None\n",
    "\n",
    "    if DERIVE_END_DATE and end_date is None and effective_date and duration:\n",
    "        # rules baseline uses a weaker derivation than hybrid, but still anchor-checked\n",
    "        derived = derive_end_date_safe(effective_date, duration, \" \".join(cands[:3]))\n",
    "        if derived:\n",
    "            end_date = derived\n",
    "            end_date_source = \"derived\"\n",
    "\n",
    "    status = \"found\" if (effective_date or end_date or duration) else (\"uncertain\" if cands else \"absent\")\n",
    "    return {\n",
    "        \"effective_date\": effective_date,\n",
    "        \"end_date\": end_date,\n",
    "        \"duration\": duration,\n",
    "        \"end_date_source\": end_date_source,\n",
    "        \"validity_status\": status\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea07047c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(doc_id: str, source_path: str, raw_text: str) -> Dict[str, Any]:\n",
    "    text = normalize_text(raw_text)\n",
    "\n",
    "    # Baselines\n",
    "    b_kw_val  = baseline_keyword(text, VALIDITY_PATTERNS)\n",
    "    b_kw_ren  = baseline_keyword(text, RENEWAL_PATTERNS)\n",
    "    b_kw_eval = baseline_keyword(text, EVAL_PATTERNS)\n",
    "\n",
    "    b_rules_validity = baseline_temporal_rules(text)\n",
    "\n",
    "    # Hybrid (SOTA-style)\n",
    "    validity = extract_validity(text)\n",
    "    renewal  = extract_renewal(text)\n",
    "    evalcl   = extract_evaluation(text)\n",
    "\n",
    "    return {\n",
    "        \"doc_id\": doc_id,\n",
    "        \"source_path\": source_path,\n",
    "\n",
    "        # Hybrid outputs\n",
    "        **validity,\n",
    "        **renewal,\n",
    "        **evalcl,\n",
    "\n",
    "        # Baselines for thesis comparison\n",
    "        \"baseline_keyword_validity\": b_kw_val,\n",
    "        \"baseline_keyword_renewal\": b_kw_ren,\n",
    "        \"baseline_keyword_eval\": b_kw_eval,\n",
    "\n",
    "        \"baseline_rules_effective_date\": b_rules_validity[\"effective_date\"],\n",
    "        \"baseline_rules_end_date\": b_rules_validity[\"end_date\"],\n",
    "        \"baseline_rules_duration\": b_rules_validity[\"duration\"],\n",
    "        \"baseline_rules_end_date_source\": b_rules_validity[\"end_date_source\"],\n",
    "        \"baseline_rules_validity_status\": b_rules_validity[\"validity_status\"],\n",
    "    }\n",
    "\n",
    "def list_agreements(root: str) -> List[Tuple[str, str]]:\n",
    "    found: Dict[str, Dict[str, str]] = {}\n",
    "    for dp, _, fnames in os.walk(root):\n",
    "        for fn in fnames:\n",
    "            low = fn.lower()\n",
    "            if not (low.endswith(\".txt\") or low.endswith(\".json\")):\n",
    "                continue\n",
    "            path = os.path.join(dp, fn)\n",
    "            doc_id = os.path.splitext(fn)[0]\n",
    "            ext = os.path.splitext(fn)[1].lower()\n",
    "            found.setdefault(doc_id, {})\n",
    "            found[doc_id][ext] = path\n",
    "\n",
    "    out = []\n",
    "    for doc_id, paths in found.items():\n",
    "        out.append((doc_id, paths[\".txt\"] if \".txt\" in paths else paths[\".json\"]))\n",
    "    out.sort(key=lambda x: x[0])\n",
    "    return out\n",
    "\n",
    "def run_poc(root: str, n: int = 10, strategy: str = \"first\", seed: int = RANDOM_SEED) -> pd.DataFrame:\n",
    "    docs = list_agreements(root)\n",
    "    if not docs:\n",
    "        raise ValueError(f\"No .txt/.json files found under: {root}\")\n",
    "\n",
    "    if len(docs) <= n:\n",
    "        chosen = docs\n",
    "    else:\n",
    "        if strategy == \"random\":\n",
    "            random.seed(seed)\n",
    "            chosen = random.sample(docs, n)\n",
    "        else:\n",
    "            chosen = docs[:n]\n",
    "\n",
    "    rows = []\n",
    "    for doc_id, path in tqdm(chosen, desc=f\"POC ({len(chosen)} agreements)\"):\n",
    "        if path.lower().endswith(\".txt\"):\n",
    "            raw_text = load_txt(path)\n",
    "        else:\n",
    "            raw_text = load_ocr_json_with_pages(path)\n",
    "\n",
    "        rows.append(process_document(doc_id, path, raw_text))\n",
    "\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f445c284",
   "metadata": {},
   "source": [
    "## 5) Batch processing and outputs\n",
    "- Iterate over a folder of OCR exports, apply the hybrid extractors, and aggregate results.\n",
    "- Output is a pandas DataFrame suitable for error analysis and thesis tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27712512",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "POC (12 agreements): 100%|██████████| 12/12 [04:09<00:00, 20.79s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>source_path</th>\n",
       "      <th>effective_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>duration</th>\n",
       "      <th>end_date_source</th>\n",
       "      <th>validity_status</th>\n",
       "      <th>validity_evidence</th>\n",
       "      <th>renewal_type</th>\n",
       "      <th>notice_period</th>\n",
       "      <th>...</th>\n",
       "      <th>evaluation_status</th>\n",
       "      <th>evaluation_evidence</th>\n",
       "      <th>baseline_keyword_validity</th>\n",
       "      <th>baseline_keyword_renewal</th>\n",
       "      <th>baseline_keyword_eval</th>\n",
       "      <th>baseline_rules_effective_date</th>\n",
       "      <th>baseline_rules_end_date</th>\n",
       "      <th>baseline_rules_duration</th>\n",
       "      <th>baseline_rules_end_date_source</th>\n",
       "      <th>baseline_rules_validity_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1October28</td>\n",
       "      <td>OCR_output/California\\1October28.txt</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>uncertain</td>\n",
       "      <td>[{'text': 'represent any obligation of funds b...</td>\n",
       "      <td>automatic</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>absent</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>uncertain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13142023</td>\n",
       "      <td>OCR_output/California\\13142023.txt</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>absent</td>\n",
       "      <td>[]</td>\n",
       "      <td>by_mutual_agreement</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>found</td>\n",
       "      <td>[{'text': 'these actions contribute to establi...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>absent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>110252023</td>\n",
       "      <td>OCR_output/California\\110252023.txt</td>\n",
       "      <td>2023-10-25</td>\n",
       "      <td>None</td>\n",
       "      <td>3 months</td>\n",
       "      <td>None</td>\n",
       "      <td>found</td>\n",
       "      <td>[{'text': 'This MOU may be modified at any tim...</td>\n",
       "      <td>by_mutual_agreement</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>found</td>\n",
       "      <td>[{'text': '5 SECTION IX Final Provisions This ...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2023-10-25</td>\n",
       "      <td>None</td>\n",
       "      <td>3 months</td>\n",
       "      <td>None</td>\n",
       "      <td>found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1September19</td>\n",
       "      <td>OCR_output/California\\1September19.txt</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>5 years</td>\n",
       "      <td>None</td>\n",
       "      <td>found</td>\n",
       "      <td>[{'text': 'consultations between the Participa...</td>\n",
       "      <td>automatic</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>found</td>\n",
       "      <td>[{'text': 'conservation, water use efficiency,...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>5 years</td>\n",
       "      <td>None</td>\n",
       "      <td>found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1August26</td>\n",
       "      <td>OCR_output/California\\1August26.txt</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>uncertain</td>\n",
       "      <td>[{'text': '3 V. CONTACT POINTS In order to ens...</td>\n",
       "      <td>by_mutual_agreement</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>absent</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>uncertain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1April30</td>\n",
       "      <td>OCR_output/California\\1April30.txt</td>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>None</td>\n",
       "      <td>explicit</td>\n",
       "      <td>found</td>\n",
       "      <td>[{'text': 'FURTHER, Upon signature by the Part...</td>\n",
       "      <td>by_mutual_agreement</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>absent</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-04-30</td>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>None</td>\n",
       "      <td>explicit</td>\n",
       "      <td>found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1April15</td>\n",
       "      <td>OCR_output/California\\1April15.txt</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>absent</td>\n",
       "      <td>[]</td>\n",
       "      <td>absent</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>absent</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>absent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1392021</td>\n",
       "      <td>OCR_output/California\\1392021.txt</td>\n",
       "      <td>2021-03-09</td>\n",
       "      <td>None</td>\n",
       "      <td>3 months</td>\n",
       "      <td>None</td>\n",
       "      <td>found</td>\n",
       "      <td>[{'text': 'A Participant that intends to withd...</td>\n",
       "      <td>by_mutual_agreement</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>found</td>\n",
       "      <td>[{'text': '4. Water policies contributing to t...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2021-03-09</td>\n",
       "      <td>None</td>\n",
       "      <td>3 months</td>\n",
       "      <td>None</td>\n",
       "      <td>found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13112022</td>\n",
       "      <td>OCR_output/California\\13112022.txt</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>uncertain</td>\n",
       "      <td>[{'text': 'A Participant who intends to withdr...</td>\n",
       "      <td>by_mutual_agreement</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>absent</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2022-03-11</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1October4 (1)</td>\n",
       "      <td>OCR_output/California\\1October4 (1).txt</td>\n",
       "      <td>2019-10-04</td>\n",
       "      <td>None</td>\n",
       "      <td>4 year</td>\n",
       "      <td>None</td>\n",
       "      <td>found</td>\n",
       "      <td>[{'text': 'such modification is to become effe...</td>\n",
       "      <td>automatic</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>found</td>\n",
       "      <td>[{'text': 'The Working Group must present a mi...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-10-04</td>\n",
       "      <td>None</td>\n",
       "      <td>4 year</td>\n",
       "      <td>None</td>\n",
       "      <td>found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>California_8</td>\n",
       "      <td>OCR_output/California\\California_8.txt</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>uncertain</td>\n",
       "      <td>[{'text': 'Such modifications will take effect...</td>\n",
       "      <td>automatic</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>found</td>\n",
       "      <td>[{'text': 'b. conduct informational workshops ...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>6 months</td>\n",
       "      <td>None</td>\n",
       "      <td>found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1May25</td>\n",
       "      <td>OCR_output/California\\1May25.txt</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>uncertain</td>\n",
       "      <td>[{'text': 'The first conference is to occur in...</td>\n",
       "      <td>absent</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>absent</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>uncertain</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           doc_id                              source_path effective_date  \\\n",
       "0      1October28     OCR_output/California\\1October28.txt           None   \n",
       "1        13142023       OCR_output/California\\13142023.txt           None   \n",
       "2       110252023      OCR_output/California\\110252023.txt     2023-10-25   \n",
       "3    1September19   OCR_output/California\\1September19.txt           None   \n",
       "4       1August26      OCR_output/California\\1August26.txt           None   \n",
       "5        1April30       OCR_output/California\\1April30.txt     2022-12-31   \n",
       "6        1April15       OCR_output/California\\1April15.txt           None   \n",
       "7         1392021        OCR_output/California\\1392021.txt     2021-03-09   \n",
       "8        13112022       OCR_output/California\\13112022.txt           None   \n",
       "9   1October4 (1)  OCR_output/California\\1October4 (1).txt     2019-10-04   \n",
       "10   California_8   OCR_output/California\\California_8.txt           None   \n",
       "11         1May25         OCR_output/California\\1May25.txt           None   \n",
       "\n",
       "      end_date  duration end_date_source validity_status  \\\n",
       "0         None      None            None       uncertain   \n",
       "1         None      None            None          absent   \n",
       "2         None  3 months            None           found   \n",
       "3         None   5 years            None           found   \n",
       "4         None      None            None       uncertain   \n",
       "5   2022-12-31      None        explicit           found   \n",
       "6         None      None            None          absent   \n",
       "7         None  3 months            None           found   \n",
       "8         None      None            None       uncertain   \n",
       "9         None    4 year            None           found   \n",
       "10        None      None            None       uncertain   \n",
       "11        None      None            None       uncertain   \n",
       "\n",
       "                                    validity_evidence         renewal_type  \\\n",
       "0   [{'text': 'represent any obligation of funds b...            automatic   \n",
       "1                                                  []  by_mutual_agreement   \n",
       "2   [{'text': 'This MOU may be modified at any tim...  by_mutual_agreement   \n",
       "3   [{'text': 'consultations between the Participa...            automatic   \n",
       "4   [{'text': '3 V. CONTACT POINTS In order to ens...  by_mutual_agreement   \n",
       "5   [{'text': 'FURTHER, Upon signature by the Part...  by_mutual_agreement   \n",
       "6                                                  []               absent   \n",
       "7   [{'text': 'A Participant that intends to withd...  by_mutual_agreement   \n",
       "8   [{'text': 'A Participant who intends to withdr...  by_mutual_agreement   \n",
       "9   [{'text': 'such modification is to become effe...            automatic   \n",
       "10  [{'text': 'Such modifications will take effect...            automatic   \n",
       "11  [{'text': 'The first conference is to occur in...               absent   \n",
       "\n",
       "   notice_period  ... evaluation_status  \\\n",
       "0           None  ...            absent   \n",
       "1           None  ...             found   \n",
       "2           None  ...             found   \n",
       "3           None  ...             found   \n",
       "4           None  ...            absent   \n",
       "5           None  ...            absent   \n",
       "6           None  ...            absent   \n",
       "7           None  ...             found   \n",
       "8           None  ...            absent   \n",
       "9           None  ...             found   \n",
       "10          None  ...             found   \n",
       "11          None  ...            absent   \n",
       "\n",
       "                                  evaluation_evidence  \\\n",
       "0                                                  []   \n",
       "1   [{'text': 'these actions contribute to establi...   \n",
       "2   [{'text': '5 SECTION IX Final Provisions This ...   \n",
       "3   [{'text': 'conservation, water use efficiency,...   \n",
       "4                                                  []   \n",
       "5                                                  []   \n",
       "6                                                  []   \n",
       "7   [{'text': '4. Water policies contributing to t...   \n",
       "8                                                  []   \n",
       "9   [{'text': 'The Working Group must present a mi...   \n",
       "10  [{'text': 'b. conduct informational workshops ...   \n",
       "11                                                 []   \n",
       "\n",
       "   baseline_keyword_validity baseline_keyword_renewal baseline_keyword_eval  \\\n",
       "0                       True                     True                 False   \n",
       "1                      False                     True                  True   \n",
       "2                       True                     True                  True   \n",
       "3                       True                     True                  True   \n",
       "4                       True                     True                 False   \n",
       "5                       True                     True                 False   \n",
       "6                      False                    False                 False   \n",
       "7                       True                     True                  True   \n",
       "8                       True                     True                 False   \n",
       "9                       True                     True                  True   \n",
       "10                      True                     True                  True   \n",
       "11                      True                    False                 False   \n",
       "\n",
       "    baseline_rules_effective_date  baseline_rules_end_date  \\\n",
       "0                            None                     None   \n",
       "1                            None                     None   \n",
       "2                      2023-10-25                     None   \n",
       "3                            None                     None   \n",
       "4                            None                     None   \n",
       "5                      2018-04-30               2022-12-31   \n",
       "6                            None                     None   \n",
       "7                      2021-03-09                     None   \n",
       "8                      2022-03-11                     None   \n",
       "9                      2019-10-04                     None   \n",
       "10                           None                     None   \n",
       "11                           None                     None   \n",
       "\n",
       "    baseline_rules_duration baseline_rules_end_date_source  \\\n",
       "0                      None                           None   \n",
       "1                      None                           None   \n",
       "2                  3 months                           None   \n",
       "3                   5 years                           None   \n",
       "4                      None                           None   \n",
       "5                      None                       explicit   \n",
       "6                      None                           None   \n",
       "7                  3 months                           None   \n",
       "8                      None                           None   \n",
       "9                    4 year                           None   \n",
       "10                 6 months                           None   \n",
       "11                     None                           None   \n",
       "\n",
       "   baseline_rules_validity_status  \n",
       "0                       uncertain  \n",
       "1                          absent  \n",
       "2                           found  \n",
       "3                           found  \n",
       "4                       uncertain  \n",
       "5                           found  \n",
       "6                          absent  \n",
       "7                           found  \n",
       "8                           found  \n",
       "9                           found  \n",
       "10                          found  \n",
       "11                      uncertain  \n",
       "\n",
       "[12 rows x 23 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = r\"OCR_output/California\"\n",
    "\n",
    "df = run_poc(root, n=12, strategy=\"random\")   # or strategy=\"random\"\n",
    "df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
